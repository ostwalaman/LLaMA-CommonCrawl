{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5de0157b",
   "metadata": {},
   "source": [
    "## 1. Configs (Paths + Threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e260593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using WET: data-v2/test_compression.warc.wet\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# -----------------------------\n",
    "# Input WET (try multiple places)\n",
    "# -----------------------------\n",
    "CANDIDATE_WET_PATHS = [\n",
    "    Path(\"/mnt/data/test_compression.warc.wet\"),   # common in mounted env\n",
    "    Path(\"data-v2/test_compression.warc.wet\"),\n",
    "    Path(\"test_compression.warc.wet\"),\n",
    "]\n",
    "\n",
    "WET_PATH = next((p for p in CANDIDATE_WET_PATHS if p.exists()), None)\n",
    "assert WET_PATH is not None, f\"Missing WET file. Tried: {CANDIDATE_WET_PATHS}\"\n",
    "print(\"✅ Using WET:\", WET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb184903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Output directory + files\n",
    "# -----------------------------\n",
    "OUT_DIR = Path(\"data-v3.1\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RAW_EXTRACT_JSONL = OUT_DIR / \"wet_raw_extracted.jsonl\"\n",
    "CLEANED_JSONL     = OUT_DIR / \"wet_cleaned_filtered.jsonl\"\n",
    "LINEDEDUP_JSONL   = OUT_DIR / \"wet_line_deduped.jsonl\"       # ✅ new\n",
    "PAGEFILTER_JSONL  = OUT_DIR / \"wet_pagefiltered.jsonl\"       # ✅ new stage\n",
    "DEDUPED_JSONL     = OUT_DIR / \"wet_deduped.jsonl\"\n",
    "FINAL_JSONL       = OUT_DIR / \"wet_final.jsonl\"              # optional stage output\n",
    "REPORT_JSON       = OUT_DIR / \"wet_report.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ca2185e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Thresholds (preserve your v2 behavior)\n",
    "# -----------------------------\n",
    "MIN_CHARS = 300\n",
    "MAX_NONASCII_RATIO = 0.25\n",
    "MAX_DIGIT_RATIO    = 0.30\n",
    "MAX_PUNCT_RATIO    = 0.35\n",
    "MIN_STOPWORD_RATIO = 0.05\n",
    "MAX_REPEAT_LINE_RATIO = 0.30\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed93bd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# New: Cross-document line-level dedup (CCNet-like approximation)\n",
    "# -----------------------------\n",
    "ENABLE_LINE_DEDUP = True\n",
    "\n",
    "# Drop lines that appear in too many documents (boilerplate templates)\n",
    "LINE_DUP_MIN_LEN = 30         # ignore tiny lines\n",
    "LINE_DUP_MAX_DOC_FREQ = 3     # was 5     # if a line appears in >= this many docs, remove it\n",
    "LINE_DEDUP_KEEP_TOPK_LONG_LINES = 0  # set >0 if you want to always keep some longest lines\n",
    "\n",
    "# -----------------------------\n",
    "# New: Page-type junk filters (high ROI)\n",
    "# -----------------------------\n",
    "ENABLE_PAGEFILTER = True\n",
    "\n",
    "# cookie/privacy dominance filter (conservative)\n",
    "COOKIE_TERMS = {\n",
    "    \"cookie\", \"cookies\", \"gdpr\", \"consent\", \"privacy\", \"policy\",\n",
    "    \"personal data\", \"legitimate interest\", \"third party\", \"preferences\"\n",
    "}\n",
    "COOKIE_MIN_HITS = 8\n",
    "COOKIE_MIN_DENSITY = 0.015     # hits / tokens\n",
    "COOKIE_MAX_UNIQUE_RATIO = 0.40 # low diversity indicates boilerplate\n",
    "\n",
    "# e-commerce / listing filter (conservative)\n",
    "ECOM_TERMS = {\n",
    "    \"add to cart\", \"checkout\", \"shipping\", \"returns\", \"sku\", \"in stock\",\n",
    "    \"out of stock\", \"buy now\", \"quantity\", \"order\", \"discount\", \"price\"\n",
    "}\n",
    "ECOM_MIN_HITS = 6\n",
    "ECOM_MIN_DENSITY = 0.012\n",
    "\n",
    "# JS/template dump filter\n",
    "JS_MARKERS = {\"function\", \"var\", \"const\", \"let\", \"document\", \"window\", \"return\"}\n",
    "JS_MIN_DENSITY = 0.02          # marker hits / tokens\n",
    "JS_MIN_PUNCT_RATIO = 0.30      # reuse your punct ratio notion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d42bfb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Optional: KenLM scoring hook (only used if you enable + have model)\n",
    "# -----------------------------\n",
    "ENABLE_KENLM = False\n",
    "KENLM_MODEL_PATH = None  # e.g., \"models/cc.en.klm\"\n",
    "KENLM_MAX_CHARS = 50000\n",
    "KENLM_SCORE_THRESHOLD = None  # e.g., -120.0 (you must calibrate; leave None to just record scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e6ac385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Optional: Reference-quality classifier (trainable proxy)\n",
    "# -----------------------------\n",
    "ENABLE_REF_QUALITY_FILTER = False\n",
    "REF_QUALITY_MODEL_PATH = OUT_DIR / \"ref_quality_model.joblib\"\n",
    "REF_QUALITY_THRESHOLD = 0.5  # probability threshold if enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30eb5e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Optional: Extended analyses (from your extended notebook)\n",
    "# -----------------------------\n",
    "RUN_NEAR_DUP = True\n",
    "RUN_TOPIC_MODELING = True\n",
    "\n",
    "NEAR_DUP_SAMPLE_N = 3000    # None = all\n",
    "MINHASH_NUM_PERM  = 128\n",
    "LSH_THRESHOLD     = 0.90\n",
    "SHINGLE_SIZE      = 5\n",
    "\n",
    "TOPIC_SAMPLE_N = 5000       # None = all\n",
    "N_TOPICS       = 8\n",
    "TOP_TERMS      = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc67c46e",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6259858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install warcio tqdm langid nltk numpy scikit-learn datasketch joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1897035d",
   "metadata": {},
   "source": [
    "## 3. Imports + NLTK Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5374720e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ostwalaman/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ostwalaman/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json, re, hashlib, random\n",
    "from collections import Counter, defaultdict\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from warcio.archiveiterator import ArchiveIterator\n",
    "import langid\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc91a6a",
   "metadata": {},
   "source": [
    "## 4. Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "807c4c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'warcinfo': 1, 'conversion': 59},\n",
       " [{'url': 'http://000af36.netsolhost.com/wordpress1/2004/09/page/2/',\n",
       "   'chars': 16478,\n",
       "   'preview': 'September | 2004 | Bob Griendling | Page 2 Menu About Home Experience Op-eds Bio Blog Top Monthly Archives: September 2004 Fantasyland Date: September 27, 2004 Author: Bob Griendling Categories: Uncategorized Funny thing'},\n",
       "  {'url': 'http://0400425.netsolhost.com/beiseker/calendar-2/action~month/exact_date~1672556400/request_format~html/',\n",
       "   'chars': 2087,\n",
       "   'preview': 'Calendar | Village of Beiseker | Page 0400425.netsolhost.com|beiseker|calendar-2|action~month|exact_date~1672556400|request_format~html| Village of Beiseker Crossroads to the Future Search Main menu Skip to primary conte'},\n",
       "  {'url': 'http://055-237-0928.com/css/0pgxv9khyq5k0ar63xv97/index.html',\n",
       "   'chars': 4542,\n",
       "   'preview': \"춘천출장만남 최신뉴스▶ 출장샵,출장마사지,출장안마 [새책]종화동안마,익산여대생출장 [새책]비천동안마,서랑동안마 대덕소개팅,웅진동안마 '지하철에서 출장30대소개팅 위험.jpg,성북출장만남 출장대행 콜걸샾 오피콜걸 여대생' 창원성인출장마사지,서대문콜걸 장수군출장만남 출장대행 콜걸샾 오피콜걸 여대생,정발산역안마 2017 국노,충청남도밤길출장샵 만다소개팅어플추천,지정면안마 고등학생 콜걸놀이터 잠잠하\"}])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WS_RE = re.compile(r\"\\s+\")\n",
    "\n",
    "def normalize_ws(text: str) -> str:\n",
    "    return WS_RE.sub(\" \", text).strip()\n",
    "\n",
    "def host_from_url(url: str) -> str:\n",
    "    try:\n",
    "        return urlparse(url).netloc.lower()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def iter_jsonl(path: Path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                yield json.loads(line)\n",
    "\n",
    "def summarize_numeric(values):\n",
    "    arr = np.array(values, dtype=float) if values else np.array([], dtype=float)\n",
    "    if arr.size == 0:\n",
    "        return {\"n\": 0}\n",
    "    return {\n",
    "        \"n\": int(arr.size),\n",
    "        \"mean\": float(arr.mean()),\n",
    "        \"p50\": float(np.median(arr)),\n",
    "        \"p90\": float(np.quantile(arr, 0.9)),\n",
    "        \"max\": float(arr.max())\n",
    "    }\n",
    "\n",
    "def analyze_raw(jsonl_path: Path, top_k_hosts=20):\n",
    "    lengths = []\n",
    "    hosts = Counter()\n",
    "    for obj in iter_jsonl(jsonl_path):\n",
    "        t = obj.get(\"content\", \"\")\n",
    "        lengths.append(len(t))\n",
    "        hosts[obj.get(\"host\", \"\")] += 1\n",
    "\n",
    "    report = {\n",
    "        \"docs\": len(lengths),\n",
    "        \"length_chars\": summarize_numeric(lengths),\n",
    "        \"top_hosts\": hosts.most_common(top_k_hosts),\n",
    "    }\n",
    "    return report\n",
    "\n",
    "def preview_wet(wet_path, max_records=60, max_preview_chars=220):\n",
    "    type_counts = Counter()\n",
    "    samples = []\n",
    "    seen = 0\n",
    "    with open(wet_path, \"rb\") as stream:\n",
    "        for rec in ArchiveIterator(stream):\n",
    "            seen += 1\n",
    "            type_counts[rec.rec_type] += 1\n",
    "            if rec.rec_type == \"conversion\":\n",
    "                url = rec.rec_headers.get_header(\"WARC-Target-URI\") or \"\"\n",
    "                raw = rec.content_stream().read()\n",
    "                text = raw.decode(\"utf-8\", errors=\"ignore\")\n",
    "                prev = normalize_ws(text[:max_preview_chars])\n",
    "                samples.append({\"url\": url, \"chars\": len(text), \"preview\": prev})\n",
    "            if seen >= max_records:\n",
    "                break\n",
    "    return {\"type_counts\": dict(type_counts), \"samples\": samples}\n",
    "\n",
    "wet_preview = preview_wet(WET_PATH, max_records=60)\n",
    "wet_preview[\"type_counts\"], wet_preview[\"samples\"][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0bdc93",
   "metadata": {},
   "source": [
    "## 5. Extract WET --> JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d58a6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting WET: 34318it [00:08, 3859.58it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_records': 34318,\n",
       " 'conversion_records': 34317,\n",
       " 'kept_docs': 32879,\n",
       " 'dropped_too_short': 1438}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_wet_to_jsonl(wet_path: Path, out_jsonl: Path, min_chars: int = 300):\n",
    "    total = 0\n",
    "    conv = 0\n",
    "    kept = 0\n",
    "    dropped_short = 0\n",
    "\n",
    "    with open(wet_path, \"rb\") as stream, open(out_jsonl, \"w\", encoding=\"utf-8\") as out:\n",
    "        for rec in tqdm(ArchiveIterator(stream), desc=\"Extracting WET\"):\n",
    "            total += 1\n",
    "            if rec.rec_type != \"conversion\":\n",
    "                continue\n",
    "\n",
    "            conv += 1\n",
    "            url = rec.rec_headers.get_header(\"WARC-Target-URI\") or \"\"\n",
    "            raw = rec.content_stream().read()\n",
    "            text_raw = raw.decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "            # Preserve your old behavior as \"content\"\n",
    "            text_norm = normalize_ws(text_raw)\n",
    "\n",
    "            if len(text_norm) < min_chars:\n",
    "                dropped_short += 1\n",
    "                continue\n",
    "\n",
    "            obj = {\n",
    "                \"url\": url,\n",
    "                \"host\": host_from_url(url),\n",
    "                \"source\": \"wet\",\n",
    "                \"content\": text_norm,     # ✅ same field you used before\n",
    "                \"content_raw\": text_raw,  # ✅ new for line-aware cleaning/dedup\n",
    "            }\n",
    "            out.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "            kept += 1\n",
    "\n",
    "    stats = {\n",
    "        \"total_records\": total,\n",
    "        \"conversion_records\": conv,\n",
    "        \"kept_docs\": kept,\n",
    "        \"dropped_too_short\": dropped_short\n",
    "    }\n",
    "    return stats\n",
    "\n",
    "extract_stats = extract_wet_to_jsonl(WET_PATH, RAW_EXTRACT_JSONL, min_chars=MIN_CHARS)\n",
    "extract_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0a052b",
   "metadata": {},
   "source": [
    "## 6. Raw Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d8e3d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'docs': 32879,\n",
       " 'length_chars': {'n': 32879,\n",
       "  'mean': 7480.538246297028,\n",
       "  'p50': 3807.0,\n",
       "  'p90': 13459.600000000002,\n",
       "  'max': 877216.0},\n",
       " 'top_hosts': [('cdha.cuny.edu', 14),\n",
       "  ('courseware.zcu.cz', 13),\n",
       "  ('turbotax.intuit.com', 10),\n",
       "  ('diecezja.pl', 9),\n",
       "  ('alcoholpolicy.niaaa.nih.gov', 8),\n",
       "  ('businessfig.com', 8),\n",
       "  ('www.besport.com', 8),\n",
       "  ('www.library.univ.kiev.ua', 7),\n",
       "  ('yscholarhub.yonsei.ac.kr', 6),\n",
       "  ('headquarters.s4.xrea.com', 5),\n",
       "  ('viavca.in2p3.fr', 5),\n",
       "  ('5ka-sale.ru', 5),\n",
       "  ('b-port.com', 5),\n",
       "  ('bryansk.news', 5),\n",
       "  ('ca.news.yahoo.com', 5),\n",
       "  ('andpremium.jp', 4),\n",
       "  ('arquivo.cienciaviva.pt', 4),\n",
       "  ('art.ceskatelevize.cz', 4),\n",
       "  ('burbujasweb.com', 4),\n",
       "  ('cleanindiajournal.com', 4)]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_report = analyze_raw(RAW_EXTRACT_JSONL)\n",
    "raw_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a4e0a1",
   "metadata": {},
   "source": [
    "## 7. Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d8d92b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAV_LINE_RE = re.compile(r\"^(menu|home|about|contact|search|skip to|privacy|terms|login|sign in)$\", re.I)\n",
    "\n",
    "def clean_doc_text_keep_lines(text_raw: str):\n",
    "    \"\"\"\n",
    "    Preserves your v2 behavior, but uses raw lines to compute repetition + boilerplate.\n",
    "    Returns:\n",
    "      cleaned_text: normalized whitespace join of kept lines\n",
    "      repeat_ratio: same idea as your v2\n",
    "      kept_lines: original kept lines (for cross-doc line dedup)\n",
    "    \"\"\"\n",
    "    raw_lines = [ln.strip() for ln in text_raw.splitlines()]\n",
    "    lines = [ln for ln in raw_lines if ln]\n",
    "\n",
    "    if not lines:\n",
    "        return \"\", 1.0, []\n",
    "\n",
    "    filtered = []\n",
    "    for ln in lines:\n",
    "        low = ln.lower().strip()\n",
    "        if len(low) <= 2:\n",
    "            continue\n",
    "        if len(low) <= 25 and NAV_LINE_RE.match(low):\n",
    "            continue\n",
    "        filtered.append(ln)\n",
    "\n",
    "    if not filtered:\n",
    "        return \"\", 1.0, []\n",
    "\n",
    "    cnt = Counter([ln.lower() for ln in filtered])\n",
    "    kept = [ln for ln in filtered if cnt[ln.lower()] < 3]  # same as your v2\n",
    "\n",
    "    # same spirit as your v2\n",
    "    repeat_ratio = 1.0 - (len(set([ln.lower() for ln in kept])) / max(1, len(kept)))\n",
    "    cleaned = normalize_ws(\"\\n\".join(kept))\n",
    "    return cleaned, float(repeat_ratio), kept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b89b6e",
   "metadata": {},
   "source": [
    "## 8. Language + Quality Gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "478066a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "DIGIT_RE = re.compile(r\"\\d\")\n",
    "PUNCT_RE = re.compile(r\"[^\\w\\s]\")\n",
    "\n",
    "def nltk_tokens(text: str):\n",
    "    return [t.lower() for t in word_tokenize(text)]\n",
    "\n",
    "def stopword_ratio_nltk(text: str):\n",
    "    toks = nltk_tokens(text)\n",
    "    words = [t for t in toks if t.isalpha()]\n",
    "    if not words:\n",
    "        return 0.0\n",
    "    sw = sum(1 for w in words if w in EN_STOPWORDS)\n",
    "    return sw / len(words)\n",
    "\n",
    "def basic_ratios(text: str):\n",
    "    if not text:\n",
    "        return {\"nonascii\": 1.0, \"digit\": 1.0, \"punct\": 1.0}\n",
    "    n = len(text)\n",
    "    nonascii = sum(1 for c in text if ord(c) > 127) / n\n",
    "    digit = len(DIGIT_RE.findall(text)) / n\n",
    "    punct = len(PUNCT_RE.findall(text)) / n\n",
    "    return {\"nonascii\": nonascii, \"digit\": digit, \"punct\": punct}\n",
    "\n",
    "def is_english_langid(text: str):\n",
    "    lang, score = langid.classify(text[:5000])\n",
    "    return (lang == \"en\"), {\"lang\": lang, \"lang_score\": float(score)}\n",
    "\n",
    "def quality_pass(text: str, repeat_line_ratio: float):\n",
    "    if len(text) < MIN_CHARS:\n",
    "        return False, {\"fail\": \"too_short\"}\n",
    "\n",
    "    r = basic_ratios(text)\n",
    "    swr = stopword_ratio_nltk(text)\n",
    "\n",
    "    if r[\"nonascii\"] > MAX_NONASCII_RATIO:\n",
    "        return False, {\"fail\":\"nonascii\", **r, \"stopword_ratio\": swr, \"repeat_line_ratio\": repeat_line_ratio}\n",
    "    if r[\"digit\"] > MAX_DIGIT_RATIO:\n",
    "        return False, {\"fail\":\"digit\", **r, \"stopword_ratio\": swr, \"repeat_line_ratio\": repeat_line_ratio}\n",
    "    if r[\"punct\"] > MAX_PUNCT_RATIO:\n",
    "        return False, {\"fail\":\"punct\", **r, \"stopword_ratio\": swr, \"repeat_line_ratio\": repeat_line_ratio}\n",
    "    if swr < MIN_STOPWORD_RATIO:\n",
    "        return False, {\"fail\":\"low_stopwords\", **r, \"stopword_ratio\": swr, \"repeat_line_ratio\": repeat_line_ratio}\n",
    "    if repeat_line_ratio > MAX_REPEAT_LINE_RATIO:\n",
    "        return False, {\"fail\":\"too_repetitive\", **r, \"stopword_ratio\": swr, \"repeat_line_ratio\": repeat_line_ratio}\n",
    "\n",
    "    return True, {**r, \"stopword_ratio\": swr, \"repeat_line_ratio\": repeat_line_ratio}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55d1456",
   "metadata": {},
   "source": [
    "## 9. Filter + Clean Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03a7b1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning+Filtering: 32879it [04:09, 131.66it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'drop_non_en': 19103,\n",
       "         'kept': 12470,\n",
       "         'drop_quality_too_repetitive': 1003,\n",
       "         'drop_quality_low_stopwords': 219,\n",
       "         'drop_quality_digit': 53,\n",
       "         'drop_quality_too_short': 30,\n",
       "         'drop_quality_nonascii': 1})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_and_clean(in_jsonl: Path, out_jsonl: Path):\n",
    "    stats = Counter()\n",
    "\n",
    "    with open(out_jsonl, \"w\", encoding=\"utf-8\") as out:\n",
    "        for obj in tqdm(iter_jsonl(in_jsonl), desc=\"Cleaning+Filtering\"):\n",
    "            content_raw = obj.get(\"content_raw\", \"\")\n",
    "            content = obj.get(\"content\", \"\")\n",
    "\n",
    "            if not content and not content_raw:\n",
    "                stats[\"drop_empty\"] += 1\n",
    "                continue\n",
    "\n",
    "            # line-aware cleaning (uses raw)\n",
    "            cleaned, rep, kept_lines = clean_doc_text_keep_lines(content_raw if content_raw else content)\n",
    "\n",
    "            if not cleaned:\n",
    "                stats[\"drop_clean_empty\"] += 1\n",
    "                continue\n",
    "\n",
    "            ok_lang, lang_meta = is_english_langid(cleaned)\n",
    "            if not ok_lang:\n",
    "                stats[\"drop_non_en\"] += 1\n",
    "                continue\n",
    "\n",
    "            ok_q, q_meta = quality_pass(cleaned, rep)\n",
    "            if not ok_q:\n",
    "                stats[f\"drop_quality_{q_meta.get('fail','unknown')}\"] += 1\n",
    "                continue\n",
    "\n",
    "            out_obj = {\n",
    "                \"url\": obj.get(\"url\", \"\"),\n",
    "                \"host\": obj.get(\"host\", \"\"),\n",
    "                \"source\": obj.get(\"source\", \"wet\"),\n",
    "                \"content\": cleaned,             # ✅ same as before\n",
    "                \"_kept_lines\": kept_lines,      # ✅ temp for cross-doc line dedup\n",
    "                \"meta\": {\"lang\": lang_meta, \"quality\": q_meta}\n",
    "            }\n",
    "            out.write(json.dumps(out_obj, ensure_ascii=False) + \"\\n\")\n",
    "            stats[\"kept\"] += 1\n",
    "\n",
    "    return stats\n",
    "\n",
    "filter_stats = filter_and_clean(RAW_EXTRACT_JSONL, CLEANED_JSONL)\n",
    "filter_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b60604",
   "metadata": {},
   "source": [
    "## 10. Cross-document line-level dedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07006912",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LineDedup pass1 (count): 12470it [00:06, 1813.46it/s]\n",
      "LineDedup pass2 (filter): 12470it [00:09, 1377.23it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'kept': 12320,\n",
       " 'drop_post_linededup_too_short': 150,\n",
       " 'docs_seen': 12470,\n",
       " 'unique_line_hashes': 389524,\n",
       " 'high_freq_lines': 5920}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LINE_NORM_RE = re.compile(r\"\\s+\")\n",
    "URL_RE = re.compile(r\"https?://\\S+|www\\.\\S+\", re.I)\n",
    "LONG_NUM_RE = re.compile(r\"\\d{2,}\")     # normalize multi-digit numbers\n",
    "HEX_RE = re.compile(r\"\\b[0-9a-f]{8,}\\b\", re.I)  # ids/hashes\n",
    "MONEY_RE = re.compile(r\"[$€£]\\s?\\d+(?:[.,]\\d+)?\")\n",
    "\n",
    "def normalize_line_for_hash(line: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize template-variant lines so boilerplate collapses together.\n",
    "    This makes cross-doc line dedup MUCH more effective.\n",
    "    \"\"\"\n",
    "    s = line.strip().lower()\n",
    "\n",
    "    # remove/normalize volatile tokens\n",
    "    s = URL_RE.sub(\"<url>\", s)\n",
    "    s = MONEY_RE.sub(\"<money>\", s)\n",
    "    s = HEX_RE.sub(\"<hex>\", s)\n",
    "    s = LONG_NUM_RE.sub(\"0\", s)  # page=123 -> page=0, dates -> 0-0-0\n",
    "\n",
    "    # collapse whitespace\n",
    "    s = LINE_NORM_RE.sub(\" \", s)\n",
    "\n",
    "    # optional: trim very long lines to avoid huge hashes on garbage\n",
    "    if len(s) > 500:\n",
    "        s = s[:500]\n",
    "\n",
    "    return s\n",
    "\n",
    "def line_level_dedup(in_jsonl: Path, out_jsonl: Path):\n",
    "    if not ENABLE_LINE_DEDUP:\n",
    "        # pass-through\n",
    "        with open(out_jsonl, \"w\", encoding=\"utf-8\") as out:\n",
    "            for obj in iter_jsonl(in_jsonl):\n",
    "                obj.pop(\"_kept_lines\", None)\n",
    "                out.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "        return {\"skipped\": True}\n",
    "\n",
    "    # Pass 1: count in how many documents each line appears (doc-frequency)\n",
    "    line_doc_freq = Counter()\n",
    "    total_docs = 0\n",
    "\n",
    "    for obj in tqdm(iter_jsonl(in_jsonl), desc=\"LineDedup pass1 (count)\"):\n",
    "        total_docs += 1\n",
    "        lines = obj.get(\"_kept_lines\", [])\n",
    "        seen_in_doc = set()\n",
    "        for ln in lines:\n",
    "            if len(ln) < LINE_DUP_MIN_LEN:\n",
    "                continue\n",
    "            h = hashlib.md5(normalize_line_for_hash(ln).encode(\"utf-8\", errors=\"ignore\")).hexdigest()\n",
    "            seen_in_doc.add(h)\n",
    "        for h in seen_in_doc:\n",
    "            line_doc_freq[h] += 1\n",
    "\n",
    "    # Pass 2: remove high-docfreq lines\n",
    "    stats = Counter()\n",
    "    with open(out_jsonl, \"w\", encoding=\"utf-8\") as out:\n",
    "        for obj in tqdm(iter_jsonl(in_jsonl), desc=\"LineDedup pass2 (filter)\"):\n",
    "            lines = obj.get(\"_kept_lines\", [])\n",
    "            kept = []\n",
    "            removed = 0\n",
    "\n",
    "            for ln in lines:\n",
    "                if len(ln) < LINE_DUP_MIN_LEN:\n",
    "                    kept.append(ln)\n",
    "                    continue\n",
    "                h = hashlib.md5(normalize_line_for_hash(ln).encode(\"utf-8\", errors=\"ignore\")).hexdigest()\n",
    "                if line_doc_freq[h] >= LINE_DUP_MAX_DOC_FREQ:\n",
    "                    removed += 1\n",
    "                else:\n",
    "                    kept.append(ln)\n",
    "\n",
    "            # optional: always keep some longest lines (if you want recall)\n",
    "            if LINE_DEDUP_KEEP_TOPK_LONG_LINES and len(kept) == 0 and len(lines) > 0:\n",
    "                kept = sorted(lines, key=len, reverse=True)[:LINE_DEDUP_KEEP_TOPK_LONG_LINES]\n",
    "\n",
    "            new_text = normalize_ws(\"\\n\".join(kept))\n",
    "            if len(new_text) < MIN_CHARS:\n",
    "                stats[\"drop_post_linededup_too_short\"] += 1\n",
    "                continue\n",
    "\n",
    "            obj[\"content\"] = new_text\n",
    "            obj[\"meta\"][\"line_dedup\"] = {\n",
    "                \"removed_lines\": int(removed),\n",
    "                \"orig_lines\": int(len(lines)),\n",
    "                \"kept_lines\": int(len(kept)),\n",
    "            }\n",
    "\n",
    "            obj.pop(\"_kept_lines\", None)  # drop temp\n",
    "            out.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "            stats[\"kept\"] += 1\n",
    "\n",
    "    stats[\"docs_seen\"] = total_docs\n",
    "    stats[\"unique_line_hashes\"] = len(line_doc_freq)\n",
    "    stats[\"high_freq_lines\"] = sum(1 for _,df in line_doc_freq.items() if df >= LINE_DUP_MAX_DOC_FREQ)\n",
    "    return dict(stats)\n",
    "\n",
    "line_dedup_stats = line_level_dedup(CLEANED_JSONL, LINEDEDUP_JSONL)\n",
    "line_dedup_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "923668fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Page-type filtering: 12320it [00:25, 490.38it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'kept': 11316,\n",
       " 'drop_cookie_privacy': 61,\n",
       " 'drop_ecommerce': 941,\n",
       " 'drop_js_dump': 2}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WORD_RE_SIMPLE = re.compile(r\"[A-Za-z]+\")\n",
    "\n",
    "def _count_phrase_hits(text_lc: str, phrases: set) -> int:\n",
    "    hits = 0\n",
    "    for p in phrases:\n",
    "        if \" \" in p:\n",
    "            hits += text_lc.count(p)\n",
    "        else:\n",
    "            # word boundary-ish: count occurrences; rough but OK\n",
    "            hits += len(re.findall(rf\"\\b{re.escape(p)}\\b\", text_lc))\n",
    "    return hits\n",
    "\n",
    "def unique_word_ratio(tokens):\n",
    "    if not tokens:\n",
    "        return 0.0\n",
    "    return len(set(tokens)) / len(tokens)\n",
    "\n",
    "def page_type_filter(in_jsonl: Path, out_jsonl: Path):\n",
    "    \"\"\"\n",
    "    Drops:\n",
    "      - cookie/privacy boilerplate-dominant pages\n",
    "      - e-commerce listing/product/checkout pages\n",
    "      - JS/template dumps\n",
    "    Runs AFTER line-dedup so boilerplate removal already happened.\n",
    "    \"\"\"\n",
    "    if not ENABLE_PAGEFILTER:\n",
    "        # pass-through\n",
    "        with open(out_jsonl, \"w\", encoding=\"utf-8\") as out:\n",
    "            for obj in iter_jsonl(in_jsonl):\n",
    "                out.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "        return {\"skipped\": True}\n",
    "\n",
    "    stats = Counter()\n",
    "\n",
    "    with open(out_jsonl, \"w\", encoding=\"utf-8\") as out:\n",
    "        for obj in tqdm(iter_jsonl(in_jsonl), desc=\"Page-type filtering\"):\n",
    "            text = obj.get(\"content\", \"\")\n",
    "            if not text:\n",
    "                stats[\"drop_empty\"] += 1\n",
    "                continue\n",
    "\n",
    "            text_lc = text.lower()\n",
    "            tokens = [t.lower() for t in WORD_RE_SIMPLE.findall(text)]\n",
    "            n_tokens = len(tokens)\n",
    "\n",
    "            if n_tokens == 0:\n",
    "                stats[\"drop_no_tokens\"] += 1\n",
    "                continue\n",
    "\n",
    "            uniq_ratio = unique_word_ratio(tokens)\n",
    "\n",
    "            # --- cookie/privacy dominance ---\n",
    "            cookie_hits = _count_phrase_hits(text_lc, COOKIE_TERMS)\n",
    "            cookie_density = cookie_hits / n_tokens\n",
    "\n",
    "            cookie_drop = (\n",
    "                cookie_hits >= COOKIE_MIN_HITS and\n",
    "                cookie_density >= COOKIE_MIN_DENSITY and\n",
    "                uniq_ratio <= COOKIE_MAX_UNIQUE_RATIO\n",
    "            )\n",
    "\n",
    "            if cookie_drop:\n",
    "                stats[\"drop_cookie_privacy\"] += 1\n",
    "                continue\n",
    "\n",
    "            # --- e-commerce/listing pages ---\n",
    "            ecom_hits = _count_phrase_hits(text_lc, ECOM_TERMS)\n",
    "            ecom_density = ecom_hits / n_tokens\n",
    "\n",
    "            ecom_drop = (\n",
    "                ecom_hits >= ECOM_MIN_HITS and\n",
    "                ecom_density >= ECOM_MIN_DENSITY\n",
    "            )\n",
    "\n",
    "            if ecom_drop:\n",
    "                stats[\"drop_ecommerce\"] += 1\n",
    "                continue\n",
    "\n",
    "            # --- JS/template dumps ---\n",
    "            js_hits = _count_phrase_hits(text_lc, JS_MARKERS)\n",
    "            js_density = js_hits / n_tokens\n",
    "\n",
    "            r = basic_ratios(text)  # uses your existing function\n",
    "\n",
    "            js_drop = (\n",
    "                js_density >= JS_MIN_DENSITY and\n",
    "                r[\"punct\"] >= JS_MIN_PUNCT_RATIO\n",
    "            )\n",
    "\n",
    "            if js_drop:\n",
    "                stats[\"drop_js_dump\"] += 1\n",
    "                continue\n",
    "\n",
    "            # keep\n",
    "            obj.setdefault(\"meta\", {})[\"pagefilter\"] = {\n",
    "                \"unique_word_ratio\": float(uniq_ratio),\n",
    "                \"cookie_hits\": int(cookie_hits),\n",
    "                \"cookie_density\": float(cookie_density),\n",
    "                \"ecom_hits\": int(ecom_hits),\n",
    "                \"ecom_density\": float(ecom_density),\n",
    "                \"js_hits\": int(js_hits),\n",
    "                \"js_density\": float(js_density),\n",
    "            }\n",
    "\n",
    "            out.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "            stats[\"kept\"] += 1\n",
    "\n",
    "    return dict(stats)\n",
    "\n",
    "pagefilter_stats = page_type_filter(LINEDEDUP_JSONL, PAGEFILTER_JSONL)\n",
    "pagefilter_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9e11a9",
   "metadata": {},
   "source": [
    "## 11. Exact Dedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18c62f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exact dedup: 11316it [00:02, 4077.35it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'kept': 11289, 'drop_exact_dup': 27, 'unique_hashes': 11289}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def content_hash(text: str) -> str:\n",
    "    t = normalize_ws(text)\n",
    "    return hashlib.md5(t.encode(\"utf-8\", errors=\"ignore\")).hexdigest()\n",
    "\n",
    "def exact_dedup(in_jsonl: Path, out_jsonl: Path):\n",
    "    seen = set()\n",
    "    stats = Counter()\n",
    "\n",
    "    with open(out_jsonl, \"w\", encoding=\"utf-8\") as out:\n",
    "        for obj in tqdm(iter_jsonl(in_jsonl), desc=\"Exact dedup\"):\n",
    "            t = obj.get(\"content\", \"\")\n",
    "            h = content_hash(t)\n",
    "            if h in seen:\n",
    "                stats[\"drop_exact_dup\"] += 1\n",
    "                continue\n",
    "            seen.add(h)\n",
    "            out.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "            stats[\"kept\"] += 1\n",
    "\n",
    "    stats[\"unique_hashes\"] = len(seen)\n",
    "    return dict(stats)\n",
    "\n",
    "DEDUP_INPUT = (\n",
    "    PAGEFILTER_JSONL if (ENABLE_LINE_DEDUP and ENABLE_PAGEFILTER)\n",
    "    else (LINEDEDUP_JSONL if ENABLE_LINE_DEDUP else CLEANED_JSONL)\n",
    ")\n",
    "\n",
    "dedup_stats = exact_dedup(DEDUP_INPUT, DEDUPED_JSONL)\n",
    "dedup_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab352b4",
   "metadata": {},
   "source": [
    "## 12. Language Distribution Probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3fd03d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LangID dist: 100%|██████████| 20000/20000 [01:23<00:00, 240.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total classified: 20000\n",
      "Top 15 languages: [('en', 8266), ('ru', 1281), ('de', 1140), ('es', 1057), ('ja', 1037), ('fr', 992), ('zh', 902), ('it', 624), ('pt', 444), ('nl', 423), ('pl', 409), ('la', 292), ('cs', 253), ('id', 252), ('vi', 244)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('en', 8266),\n",
       " ('ru', 1281),\n",
       " ('de', 1140),\n",
       " ('es', 1057),\n",
       " ('ja', 1037),\n",
       " ('fr', 992),\n",
       " ('zh', 902),\n",
       " ('it', 624),\n",
       " ('pt', 444),\n",
       " ('nl', 423)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lang_distribution(jsonl_path: Path, sample_n=20000, seed=42):\n",
    "    cnt = Counter()\n",
    "    rows = list(iter_jsonl(jsonl_path))\n",
    "    random.Random(seed).shuffle(rows)\n",
    "    rows = rows[:min(sample_n, len(rows))]\n",
    "\n",
    "    for obj in tqdm(rows, desc=\"LangID dist\"):\n",
    "        text = obj.get(\"content\", \"\")\n",
    "        if not text:\n",
    "            continue\n",
    "        lang, score = langid.classify(text[:5000])\n",
    "        cnt[lang] += 1\n",
    "\n",
    "    print(\"Total classified:\", sum(cnt.values()))\n",
    "    print(\"Top 15 languages:\", cnt.most_common(15))\n",
    "    return cnt\n",
    "\n",
    "lang_cnt = lang_distribution(RAW_EXTRACT_JSONL, sample_n=20000)\n",
    "lang_cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3257681e",
   "metadata": {},
   "source": [
    "## 13. KenLM Scoring Hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c259c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install https://github.com/kpu/kenlm/archive/master.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23c5cfd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'skipped': True}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def try_load_kenlm(model_path):\n",
    "    try:\n",
    "        import kenlm\n",
    "    except Exception as e:\n",
    "        print(\"KenLM not available. Install it first if you want this stage.\")\n",
    "        return None\n",
    "    if model_path is None:\n",
    "        print(\"KENLM_MODEL_PATH is None. Provide a path to a .klm model.\")\n",
    "        return None\n",
    "    try:\n",
    "        return kenlm.Model(str(model_path))\n",
    "    except Exception as e:\n",
    "        print(\"Failed to load KenLM model:\", e)\n",
    "        return None\n",
    "\n",
    "def kenlm_score_text(model, text: str):\n",
    "    # kenlm.Model.score returns log10 probability by default (depends on build)\n",
    "    # We record the raw score; you must calibrate thresholds on your corpus.\n",
    "    text = text[:KENLM_MAX_CHARS]\n",
    "    return float(model.score(text))\n",
    "\n",
    "def apply_kenlm_filter(in_jsonl: Path, out_jsonl: Path):\n",
    "    if not ENABLE_KENLM:\n",
    "        # pass-through\n",
    "        with open(out_jsonl, \"w\", encoding=\"utf-8\") as out:\n",
    "            for obj in iter_jsonl(in_jsonl):\n",
    "                out.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "        return {\"skipped\": True}\n",
    "\n",
    "    model = try_load_kenlm(KENLM_MODEL_PATH)\n",
    "    if model is None:\n",
    "        # pass-through but warn\n",
    "        with open(out_jsonl, \"w\", encoding=\"utf-8\") as out:\n",
    "            for obj in iter_jsonl(in_jsonl):\n",
    "                out.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "        return {\"skipped\": True, \"reason\": \"kenlm_not_loaded\"}\n",
    "\n",
    "    stats = Counter()\n",
    "    with open(out_jsonl, \"w\", encoding=\"utf-8\") as out:\n",
    "        for obj in tqdm(iter_jsonl(in_jsonl), desc=\"KenLM scoring\"):\n",
    "            t = obj.get(\"content\", \"\")\n",
    "            s = kenlm_score_text(model, t)\n",
    "            obj.setdefault(\"meta\", {}).setdefault(\"quality\", {})[\"kenlm_score\"] = s\n",
    "\n",
    "            if KENLM_SCORE_THRESHOLD is not None and s < KENLM_SCORE_THRESHOLD:\n",
    "                stats[\"drop_kenlm_low_score\"] += 1\n",
    "                continue\n",
    "\n",
    "            out.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "            stats[\"kept\"] += 1\n",
    "\n",
    "    return dict(stats)\n",
    "\n",
    "kenlm_stats = apply_kenlm_filter(DEDUPED_JSONL, FINAL_JSONL)\n",
    "kenlm_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f270cb",
   "metadata": {},
   "source": [
    "## 14. Optional Reference-Quality Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69a8a861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "WORD_RE = re.compile(r\"[A-Za-z]+\")\n",
    "\n",
    "def featurize_doc(text: str, host: str = \"\") -> dict:\n",
    "    r = basic_ratios(text)\n",
    "    swr = stopword_ratio_nltk(text)\n",
    "    words = WORD_RE.findall(text)\n",
    "    n_words = len(words)\n",
    "    avg_word_len = (sum(len(w) for w in words) / n_words) if n_words else 0.0\n",
    "    return {\n",
    "        \"len_chars\": len(text),\n",
    "        \"n_words\": n_words,\n",
    "        \"avg_word_len\": avg_word_len,\n",
    "        \"stopword_ratio\": swr,\n",
    "        \"nonascii_ratio\": r[\"nonascii\"],\n",
    "        \"digit_ratio\": r[\"digit\"],\n",
    "        \"punct_ratio\": r[\"punct\"],\n",
    "        \"host_tld\": host.split(\".\")[-1] if host and \".\" in host else host,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1a88fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ref_quality_classifier(labeled_rows):\n",
    "    \"\"\"\n",
    "    labeled_rows: list of dicts with keys: content, host, label (0/1)\n",
    "    \"\"\"\n",
    "    X = [featurize_doc(r[\"content\"], r.get(\"host\",\"\")) for r in labeled_rows]\n",
    "    y = [int(r[\"label\"]) for r in labeled_rows]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y if len(set(y))>1 else None)\n",
    "\n",
    "    clf = Pipeline([\n",
    "        (\"vec\", DictVectorizer(sparse=True)),\n",
    "        (\"lr\", LogisticRegression(max_iter=200, n_jobs=None))\n",
    "    ])\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_test)\n",
    "    print(classification_report(y_test, preds))\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b303091a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ref-quality stage ready. To use it:\n",
      "1) Create labeled_rows (manual or weak labels)\n",
      "2) clf = train_ref_quality_classifier(labeled_rows)\n",
      "3) joblib.dump(clf, REF_QUALITY_MODEL_PATH)\n",
      "4) ENABLE_REF_QUALITY_FILTER=True and run apply_ref_quality_filter(...)\n"
     ]
    }
   ],
   "source": [
    "def apply_ref_quality_filter(in_jsonl: Path, out_jsonl: Path, model_path: Path, threshold: float = 0.5):\n",
    "    if not ENABLE_REF_QUALITY_FILTER:\n",
    "        with open(out_jsonl, \"w\", encoding=\"utf-8\") as out:\n",
    "            for obj in iter_jsonl(in_jsonl):\n",
    "                out.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "        return {\"skipped\": True}\n",
    "\n",
    "    if not model_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing ref-quality model at: {model_path}\")\n",
    "\n",
    "    clf = joblib.load(model_path)\n",
    "    stats = Counter()\n",
    "\n",
    "    with open(out_jsonl, \"w\", encoding=\"utf-8\") as out:\n",
    "        for obj in tqdm(iter_jsonl(in_jsonl), desc=\"Ref-quality filter\"):\n",
    "            t = obj.get(\"content\", \"\")\n",
    "            host = obj.get(\"host\",\"\")\n",
    "            feats = featurize_doc(t, host)\n",
    "            proba = float(clf.predict_proba([feats])[0][1])\n",
    "            obj.setdefault(\"meta\", {})[\"ref_quality_proba\"] = proba\n",
    "\n",
    "            if proba < threshold:\n",
    "                stats[\"drop_ref_quality_low\"] += 1\n",
    "                continue\n",
    "\n",
    "            out.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "            stats[\"kept\"] += 1\n",
    "\n",
    "    return dict(stats)\n",
    "\n",
    "print(\"✅ Ref-quality stage ready. To use it:\")\n",
    "print(\"1) Create labeled_rows (manual or weak labels)\")\n",
    "print(\"2) clf = train_ref_quality_classifier(labeled_rows)\")\n",
    "print(\"3) joblib.dump(clf, REF_QUALITY_MODEL_PATH)\")\n",
    "print(\"4) ENABLE_REF_QUALITY_FILTER=True and run apply_ref_quality_filter(...)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e472b1d2",
   "metadata": {},
   "source": [
    "## 15. Near-duplicate clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ce07307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs for near-dup: 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building MinHashes: 100%|██████████| 3000/3000 [00:17<00:00, 167.66it/s]\n",
      "Query LSH: 100%|██████████| 3000/3000 [00:00<00:00, 185080.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total clusters: 2998\n",
      "Clusters size>=2: 2\n",
      "Top 10 cluster sizes: [2, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "Cluster 1 size=2\n",
      " - https://www.mjtunes.com/modules/mydownloads/singlefile.php?lid=2887 | preview: Michael Jackson - Who Is The Thriller (Mashup) / Remixes Michael Jackson Music database - MjTunes Listen to MjTunes Radio You can VOTE for and RATE your favorite tracks. Vote only\n",
      " - http://www.mjtunes.com/modules/mydownloads/singlefile.php?lid=3218 | preview: Michael Jackson - Earth Song (DJ Gnome Dubstep Remix) / Remixes Michael Jackson Music database - MjTunes Listen to MjTunes Radio You can VOTE for and RATE your favorite tracks. Vot\n",
      "\n",
      "Cluster 2 size=2\n",
      " - https://turbotax.intuit.com/reviews/online/premium/?page=1656 | preview: Skip To Main Content Must file by 3/31. Start for free expand navigation options Expert does your taxes Expert does your taxes Do it yourself Do it yourself We’ll guide you step-by\n",
      " - https://turbotax.intuit.com/reviews/online/deluxe/?page=10982 | preview: Skip To Main Content Must file by 3/31. Start for free expand navigation options Expert does your taxes Expert does your taxes Do it yourself Do it yourself We’ll guide you step-by\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'docs_used': 3000,\n",
       " 'clusters_total': 2998,\n",
       " 'clusters_ge2': 2,\n",
       " 'top_cluster_sizes': [2, 2, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasketch import MinHash, MinHashLSH\n",
    "\n",
    "def load_docs(jsonl_path: Path, limit=None, seed=42):\n",
    "    docs = list(iter_jsonl(jsonl_path))\n",
    "    if limit is not None and len(docs) > limit:\n",
    "        rng = random.Random(seed)\n",
    "        rng.shuffle(docs)\n",
    "        docs = docs[:limit]\n",
    "    return docs\n",
    "\n",
    "def word_shingles(text: str, k=5):\n",
    "    words = [w.lower() for w in WORD_RE.findall(text)]\n",
    "    if len(words) < k:\n",
    "        return []\n",
    "    return [\" \".join(words[i:i+k]) for i in range(len(words)-k+1)]\n",
    "\n",
    "def build_minhash(shingles, num_perm=128):\n",
    "    m = MinHash(num_perm=num_perm)\n",
    "    for s in shingles:\n",
    "        m.update(s.encode(\"utf-8\", errors=\"ignore\"))\n",
    "    return m\n",
    "\n",
    "class UnionFind:\n",
    "    def __init__(self, n):\n",
    "        self.p = list(range(n))\n",
    "        self.r = [0]*n\n",
    "    def find(self, x):\n",
    "        while self.p[x] != x:\n",
    "            self.p[x] = self.p[self.p[x]]\n",
    "            x = self.p[x]\n",
    "        return x\n",
    "    def union(self, a, b):\n",
    "        ra, rb = self.find(a), self.find(b)\n",
    "        if ra == rb:\n",
    "            return\n",
    "        if self.r[ra] < self.r[rb]:\n",
    "            self.p[ra] = rb\n",
    "        elif self.r[ra] > self.r[rb]:\n",
    "            self.p[rb] = ra\n",
    "        else:\n",
    "            self.p[rb] = ra\n",
    "            self.r[ra] += 1\n",
    "\n",
    "def near_duplicate_clusters(jsonl_path: Path):\n",
    "    if not RUN_NEAR_DUP:\n",
    "        return {\"skipped\": True}\n",
    "\n",
    "    docs = load_docs(jsonl_path, limit=NEAR_DUP_SAMPLE_N, seed=RANDOM_SEED)\n",
    "    texts = [d.get(\"content\",\"\") for d in docs]\n",
    "    print(\"Docs for near-dup:\", len(texts))\n",
    "\n",
    "    lsh = MinHashLSH(threshold=LSH_THRESHOLD, num_perm=MINHASH_NUM_PERM)\n",
    "    minhashes = []\n",
    "    for i, t in enumerate(tqdm(texts, desc=\"Building MinHashes\")):\n",
    "        shingles = word_shingles(t, k=SHINGLE_SIZE)\n",
    "        mh = build_minhash(shingles, num_perm=MINHASH_NUM_PERM) if shingles else build_minhash([\"\"], num_perm=MINHASH_NUM_PERM)\n",
    "        minhashes.append(mh)\n",
    "        lsh.insert(str(i), mh)\n",
    "\n",
    "    uf = UnionFind(len(texts))\n",
    "    for i, mh in enumerate(tqdm(minhashes, desc=\"Query LSH\")):\n",
    "        hits = lsh.query(mh)\n",
    "        for h in hits:\n",
    "            j = int(h)\n",
    "            if i != j:\n",
    "                uf.union(i, j)\n",
    "\n",
    "    clusters = defaultdict(list)\n",
    "    for i in range(len(texts)):\n",
    "        clusters[uf.find(i)].append(i)\n",
    "\n",
    "    cluster_sizes = sorted([len(v) for v in clusters.values()], reverse=True)\n",
    "    big = [v for v in clusters.values() if len(v) >= 2]\n",
    "    print(\"Total clusters:\", len(clusters))\n",
    "    print(\"Clusters size>=2:\", len(big))\n",
    "    print(\"Top 10 cluster sizes:\", cluster_sizes[:10])\n",
    "\n",
    "    # show a few examples\n",
    "    for idx, members in enumerate(sorted(big, key=len, reverse=True)[:5], 1):\n",
    "        print(f\"\\nCluster {idx} size={len(members)}\")\n",
    "        for m in members[:2]:\n",
    "            print(\" -\", docs[m].get(\"url\",\"\"), \"| preview:\", normalize_ws(texts[m][:180]))\n",
    "\n",
    "    return {\n",
    "        \"docs_used\": len(texts),\n",
    "        \"clusters_total\": len(clusters),\n",
    "        \"clusters_ge2\": len(big),\n",
    "        \"top_cluster_sizes\": cluster_sizes[:10]\n",
    "    }\n",
    "\n",
    "near_dup_report = near_duplicate_clusters(DEDUPED_JSONL if not ENABLE_KENLM else FINAL_JSONL)\n",
    "near_dup_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4ad411",
   "metadata": {},
   "source": [
    "## 16. Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e8b90a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs for topic modeling: 5000\n",
      "\n",
      "Topic terms:\n",
      "topic_0 : ['services', 'business', 'health', 'research', 'school', 'management', 'university', 'education', 'data', 'information', 'news', 'service']\n",
      "topic_1 : ['november', 'april', 'january', 'october', 'september', 'june', 'december', 'march', 'july', 'february', 'august', '2019']\n",
      "topic_2 : ['cart', 'shop', 'accessories', 'products', '00', 'product', 'usd', 'shipping', 'size', 'sale', 'add', '99']\n",
      "topic_3 : ['function', 'var', 'return', 'data', 'function var', 'document', 'function return', 'window', 'jquery', 'null', 'length', 'const']\n",
      "topic_4 : ['like', 'just', 'time', 'people', 'best', 'love', 'game', 'life', 'day', 'book', 'know', 'new']\n",
      "topic_5 : ['00', 'pm', '00 pm', '12', '11', '10', '00 00', 'views', '30', '2024', '14', '20']\n",
      "topic_6 : ['islands', 'republic', 'guinea', 'saint', 'united', 'eur', 'usd', 'south', 'island', 'arab', 'congo', 'french']\n",
      "topic_7 : ['search', 'password', 'cookies', 'privacy', 'forum', 'email', 'policy', 'log', 'page', 'terms', 'account', 'sign']\n",
      "\n",
      "Dominant topic counts:\n",
      "topic_4: 1018\n",
      "topic_0: 1018\n",
      "topic_7: 1005\n",
      "topic_2: 858\n",
      "topic_5: 468\n",
      "topic_1: 383\n",
      "topic_3: 145\n",
      "topic_6: 105\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'docs_used': 5000,\n",
       " 'n_topics': 8,\n",
       " 'topic_terms': {'topic_0': ['services',\n",
       "   'business',\n",
       "   'health',\n",
       "   'research',\n",
       "   'school',\n",
       "   'management',\n",
       "   'university',\n",
       "   'education',\n",
       "   'data',\n",
       "   'information',\n",
       "   'news',\n",
       "   'service'],\n",
       "  'topic_1': ['november',\n",
       "   'april',\n",
       "   'january',\n",
       "   'october',\n",
       "   'september',\n",
       "   'june',\n",
       "   'december',\n",
       "   'march',\n",
       "   'july',\n",
       "   'february',\n",
       "   'august',\n",
       "   '2019'],\n",
       "  'topic_2': ['cart',\n",
       "   'shop',\n",
       "   'accessories',\n",
       "   'products',\n",
       "   '00',\n",
       "   'product',\n",
       "   'usd',\n",
       "   'shipping',\n",
       "   'size',\n",
       "   'sale',\n",
       "   'add',\n",
       "   '99'],\n",
       "  'topic_3': ['function',\n",
       "   'var',\n",
       "   'return',\n",
       "   'data',\n",
       "   'function var',\n",
       "   'document',\n",
       "   'function return',\n",
       "   'window',\n",
       "   'jquery',\n",
       "   'null',\n",
       "   'length',\n",
       "   'const'],\n",
       "  'topic_4': ['like',\n",
       "   'just',\n",
       "   'time',\n",
       "   'people',\n",
       "   'best',\n",
       "   'love',\n",
       "   'game',\n",
       "   'life',\n",
       "   'day',\n",
       "   'book',\n",
       "   'know',\n",
       "   'new'],\n",
       "  'topic_5': ['00',\n",
       "   'pm',\n",
       "   '00 pm',\n",
       "   '12',\n",
       "   '11',\n",
       "   '10',\n",
       "   '00 00',\n",
       "   'views',\n",
       "   '30',\n",
       "   '2024',\n",
       "   '14',\n",
       "   '20'],\n",
       "  'topic_6': ['islands',\n",
       "   'republic',\n",
       "   'guinea',\n",
       "   'saint',\n",
       "   'united',\n",
       "   'eur',\n",
       "   'usd',\n",
       "   'south',\n",
       "   'island',\n",
       "   'arab',\n",
       "   'congo',\n",
       "   'french'],\n",
       "  'topic_7': ['search',\n",
       "   'password',\n",
       "   'cookies',\n",
       "   'privacy',\n",
       "   'forum',\n",
       "   'email',\n",
       "   'policy',\n",
       "   'log',\n",
       "   'page',\n",
       "   'terms',\n",
       "   'account',\n",
       "   'sign']},\n",
       " 'dominant_topic_counts': {4: 1018,\n",
       "  2: 858,\n",
       "  3: 145,\n",
       "  0: 1018,\n",
       "  1: 383,\n",
       "  5: 468,\n",
       "  7: 1005,\n",
       "  6: 105}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "def topic_modeling(jsonl_path: Path):\n",
    "    if not RUN_TOPIC_MODELING:\n",
    "        return {\"skipped\": True}\n",
    "\n",
    "    docs_tm = load_docs(jsonl_path, limit=TOPIC_SAMPLE_N, seed=RANDOM_SEED)\n",
    "    texts = [d[\"content\"] for d in docs_tm]\n",
    "    print(\"Docs for topic modeling:\", len(texts))\n",
    "\n",
    "    if len(texts) < 20:\n",
    "        print(\"Not enough documents to run topic modeling reliably.\")\n",
    "        return {\"docs_used\": len(texts), \"skipped\": True, \"reason\": \"too_few_docs\"}\n",
    "\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=30000,\n",
    "        min_df=2,\n",
    "        max_df=0.9,\n",
    "        ngram_range=(1,2),\n",
    "        stop_words=\"english\"\n",
    "    )\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "\n",
    "    nmf = NMF(n_components=N_TOPICS, random_state=RANDOM_SEED)\n",
    "    W = nmf.fit_transform(X)\n",
    "    H = nmf.components_\n",
    "    terms = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "    def top_terms(topic_idx, topn=12):\n",
    "        idx = np.argsort(H[topic_idx])[::-1][:topn]\n",
    "        return terms[idx].tolist()\n",
    "\n",
    "    topic_terms = {f\"topic_{i}\": top_terms(i, TOP_TERMS) for i in range(N_TOPICS)}\n",
    "    dominant = W.argmax(axis=1)\n",
    "\n",
    "    print(\"\\nTopic terms:\")\n",
    "    for k, v in topic_terms.items():\n",
    "        print(k, \":\", v)\n",
    "\n",
    "    print(\"\\nDominant topic counts:\")\n",
    "    dom_cnt = Counter(dominant.tolist())\n",
    "    for t, c in dom_cnt.most_common():\n",
    "        print(f\"topic_{t}: {c}\")\n",
    "\n",
    "    return {\n",
    "        \"docs_used\": len(texts),\n",
    "        \"n_topics\": N_TOPICS,\n",
    "        \"topic_terms\": topic_terms,\n",
    "        \"dominant_topic_counts\": dict(dom_cnt)\n",
    "    }\n",
    "\n",
    "topic_report = topic_modeling(DEDUPED_JSONL if not ENABLE_KENLM else FINAL_JSONL)\n",
    "topic_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cda8e1",
   "metadata": {},
   "source": [
    "## 17. Final JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f6d7514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved report: data-v3.1/wet_report.json\n",
      "✅ Outputs in: /Users/ostwalaman/Desktop/LLaMA Project/data-v3.1\n"
     ]
    }
   ],
   "source": [
    "post_report = analyze_raw(DEDUPED_JSONL if not ENABLE_KENLM else FINAL_JSONL)\n",
    "\n",
    "final_report = {\n",
    "    \"input_wet\": str(WET_PATH),\n",
    "    \"raw_extract_jsonl\": str(RAW_EXTRACT_JSONL),\n",
    "    \"cleaned_jsonl\": str(CLEANED_JSONL),\n",
    "    \"line_deduped_jsonl\": str(LINEDEDUP_JSONL),\n",
    "    \"deduped_jsonl\": str(DEDUPED_JSONL),\n",
    "    \"final_jsonl\": str(FINAL_JSONL),\n",
    "    \"raw_report\": raw_report,\n",
    "    \"extract_stats\": extract_stats,\n",
    "    \"filter_stats\": dict(filter_stats),\n",
    "    \"line_dedup_stats\": line_dedup_stats,\n",
    "    \"exact_dedup_stats\": dict(dedup_stats),\n",
    "    \"near_dup_report\": near_dup_report,\n",
    "    \"topic_report\": topic_report,\n",
    "    \"post_report\": post_report,\n",
    "    \"pagefilter_jsonl\": str(PAGEFILTER_JSONL),\n",
    "    \"pagefilter_stats\": dict(pagefilter_stats),\n",
    "    \"notes\": {\n",
    "        \"tokenization\": \"NLTK word_tokenize + NLTK stopwords used for stopword_ratio\",\n",
    "        \"language_id\": \"langid (same as v2)\",\n",
    "        \"dedup_exact\": \"md5 of whitespace-normalized content\",\n",
    "        \"line_level_dedup\": \"cross-document doc-frequency line removal (CCNet-like approximation)\",\n",
    "        \"kenlm\": \"optional scoring hook (disabled by default)\",\n",
    "        \"ref_quality\": \"optional classifier proxy (disabled by default)\",\n",
    "        \"pagefilter\": \"cookie/privacy + ecommerce + js template page-type filters (conservative)\"\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(REPORT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(final_report, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✅ Saved report:\", REPORT_JSON)\n",
    "print(\"✅ Outputs in:\", OUT_DIR.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ae81be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
