{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17f25aef",
   "metadata": {},
   "source": [
    "## Config + Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0ca414",
   "metadata": {},
   "source": [
    "## Data access (for class/demo)\n",
    "\n",
    "- **Sample WET file location:** the notebook looks for `test_compression.warc.wet` in:\n",
    "  - `/mnt/data/test_compression.warc.wet` (recommended for Colab uploads)\n",
    "  - `data/test_compression.warc.wet`\n",
    "  - `test_compression.warc.wet`\n",
    "\n",
    "- **Generated artifacts (written to `./data/`):**\n",
    "  - `wet_raw_extracted.jsonl` (raw extracted conversion text)\n",
    "  - `wet_cleaned_filtered.jsonl` (cleaned + English + quality filtered)\n",
    "  - `wet_deduped.jsonl` (exact deduplicated)\n",
    "  - `wet_report.json` (summary stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cfa8825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using WET file: data-v2/test_compression.warc.wet\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Input WET\n",
    "# Place your sample WET file in ONE of these locations:\n",
    "#   1) /mnt/data/test_compression.warc.wet   (Colab / mounted uploads)\n",
    "#   2) data/test_compression.warc.wet       (repo-relative)\n",
    "#   3) test_compression.warc.wet            (repo root)\n",
    "\n",
    "CANDIDATE_WET_PATHS = [\n",
    "    Path(\"data-v2/test_compression.warc.wet\"),\n",
    "    # Path(\"data/test_compression.warc.wet\"),\n",
    "    # Path(\"test_compression.warc.wet\"),\n",
    "]\n",
    "\n",
    "WET_PATH = next((p for p in CANDIDATE_WET_PATHS if p.exists()), None)\n",
    "assert WET_PATH is not None, f\"Missing WET file. Tried: {CANDIDATE_WET_PATHS}\"\n",
    "print(\"✅ Using WET file:\", WET_PATH)\n",
    "\n",
    "# Output directory\n",
    "OUT_DIR = Path(\"data-v2\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Output files\n",
    "RAW_EXTRACT_JSONL = OUT_DIR / \"wet_raw_extracted.jsonl\"\n",
    "CLEANED_JSONL     = OUT_DIR / \"wet_cleaned_filtered.jsonl\"\n",
    "DEDUPED_JSONL      = OUT_DIR / \"wet_deduped.jsonl\"\n",
    "REPORT_JSON        = OUT_DIR / \"wet_report.json\"\n",
    "\n",
    "# Thresholds\n",
    "MIN_CHARS = 300\n",
    "MAX_NONASCII_RATIO = 0.25\n",
    "MAX_DIGIT_RATIO    = 0.30\n",
    "MAX_PUNCT_RATIO    = 0.35\n",
    "MIN_STOPWORD_RATIO = 0.05\n",
    "MAX_REPEAT_LINE_RATIO = 0.30\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e44eed4",
   "metadata": {},
   "source": [
    "## Install + imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9863ee14",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install warcio tqdm langid nltk numpy scikit-learn datasketch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8feaf08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re, hashlib, random\n",
    "from collections import Counter, defaultdict\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from warcio.archiveiterator import ArchiveIterator\n",
    "import langid\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3981dac5",
   "metadata": {},
   "source": [
    "## Download NLTK resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5be40809",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ostwalaman/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ostwalaman/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca89dcbf",
   "metadata": {},
   "source": [
    "## WET sanity scan (first N records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "668be4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scanned first 60 records\n",
      "Record types: {'warcinfo': 1, 'conversion': 59}\n",
      "\n",
      "--- conversion samples (up to 5) ---\n",
      "\n",
      "[1] chars=16478\n",
      "URL: http://000af36.netsolhost.com/wordpress1/2004/09/page/2/\n",
      "Preview: September | 2004 | Bob Griendling | Page 2 Menu About Home Experience Op-eds Bio Blog Top Monthly Archives: September 2004 Fantasyland Date: September 27, 2004 Author: Bob Griendling Categories: Uncategorized Funny thing\n",
      "\n",
      "[2] chars=2087\n",
      "URL: http://0400425.netsolhost.com/beiseker/calendar-2/action~month/exact_date~1672556400/request_format~html/\n",
      "Preview: Calendar | Village of Beiseker | Page 0400425.netsolhost.com|beiseker|calendar-2|action~month|exact_date~1672556400|request_format~html| Village of Beiseker Crossroads to the Future Search Main menu Skip to primary conte\n",
      "\n",
      "[3] chars=4542\n",
      "URL: http://055-237-0928.com/css/0pgxv9khyq5k0ar63xv97/index.html\n",
      "Preview: 춘천출장만남 최신뉴스▶ 출장샵,출장마사지,출장안마 [새책]종화동안마,익산여대생출장 [새책]비천동안마,서랑동안마 대덕소개팅,웅진동안마 '지하철에서 출장30대소개팅 위험.jpg,성북출장만남 출장대행 콜걸샾 오피콜걸 여대생' 창원성인출장마사지,서대문콜걸 장수군출장만남 출장대행 콜걸샾 오피콜걸 여대생,정발산역안마 2017 국노,충청남도밤길출장샵 만다소개팅어플추천,지정면안마 고등학생 콜걸놀이터 잠잠하\n",
      "\n",
      "[4] chars=24302\n",
      "URL: http://07621.de/00.de/wp-includes/pomo/ebook.php?q=ebook-Business-of-Biotechnology.-From-the-Bench-to-the-Street/\n",
      "Preview: Ebook Business Of Biotechnology. From The Bench To The Street Ebook Business Of Biotechnology. From The Bench To The Street by Tommy 3.2 Between 1840 and 1860, neural choices was Fridays in adopting an new ebook Business\n",
      "\n",
      "[5] chars=1405\n",
      "URL: http://0778.org/info_67.html\n",
      "Preview: 【整栋出租】大化县江滨北路32号，五层半整栋招租 电话：18977801258 - 大化镇 - 门市/商铺/办公用房 - 大化信息网 大化信息网(0778.org)正式上线试运营！欢迎广大父老乡亲奔走相告，各类供求信息免费发布与查阅！ 隐私政策 服务协议 关于网站 联系我们 免费发布信息[非常简单] 免费发布信息 首页 招聘求职 供求信息 二手市场 车辆信息 房产信息 生活服务 商务服务 教育培训 征婚交友 当前位置：首页 > 门市/商\n"
     ]
    }
   ],
   "source": [
    "WS_RE = re.compile(r\"\\s+\")\n",
    "\n",
    "def normalize_ws(text: str) -> str:\n",
    "    return WS_RE.sub(\" \", text).strip()\n",
    "\n",
    "def preview_wet(wet_path, max_records=60, max_preview_chars=220):\n",
    "    type_counts = Counter()\n",
    "    samples = []\n",
    "\n",
    "    seen = 0\n",
    "    with open(wet_path, \"rb\") as stream:\n",
    "        for rec in ArchiveIterator(stream):\n",
    "            seen += 1\n",
    "            type_counts[rec.rec_type] += 1\n",
    "\n",
    "            if rec.rec_type == \"conversion\":\n",
    "                url = rec.rec_headers.get_header(\"WARC-Target-URI\") or \"\"\n",
    "                raw = rec.content_stream().read()\n",
    "                text = raw.decode(\"utf-8\", errors=\"ignore\")\n",
    "                prev = normalize_ws(text[:max_preview_chars])\n",
    "                samples.append({\"url\": url, \"chars\": len(text), \"preview\": prev})\n",
    "\n",
    "            if seen >= max_records:\n",
    "                break\n",
    "\n",
    "    print(f\"✅ Scanned first {seen} records\")\n",
    "    print(\"Record types:\", dict(type_counts))\n",
    "    print(\"\\n--- conversion samples (up to 5) ---\")\n",
    "    for i, s in enumerate(samples[:5], 1):\n",
    "        print(f\"\\n[{i}] chars={s['chars']}\")\n",
    "        print(\"URL:\", s[\"url\"])\n",
    "        print(\"Preview:\", s[\"preview\"])\n",
    "\n",
    "preview_wet(WET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d509e9d3",
   "metadata": {},
   "source": [
    "## Extract conversion records → wet_raw_extracted.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6cfbdb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting WET: 34318it [00:09, 3795.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Extraction complete\n",
      "Total records      : 34318\n",
      "Conversion records : 34317\n",
      "Kept docs          : 32879\n",
      "Dropped too short  : 1438\n",
      "Saved to           : data-v2/wet_raw_extracted.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def host_from_url(url: str) -> str:\n",
    "    try:\n",
    "        return urlparse(url).netloc.lower()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def extract_wet_to_jsonl(wet_path: Path, out_jsonl: Path, min_chars: int = 300):\n",
    "    total = 0\n",
    "    conv = 0\n",
    "    kept = 0\n",
    "    dropped_short = 0\n",
    "\n",
    "    with open(wet_path, \"rb\") as stream, open(out_jsonl, \"w\", encoding=\"utf-8\") as out:\n",
    "        for rec in tqdm(ArchiveIterator(stream), desc=\"Extracting WET\"):\n",
    "            total += 1\n",
    "            if rec.rec_type != \"conversion\":\n",
    "                continue\n",
    "\n",
    "            conv += 1\n",
    "            url = rec.rec_headers.get_header(\"WARC-Target-URI\") or \"\"\n",
    "            raw = rec.content_stream().read()\n",
    "            text = raw.decode(\"utf-8\", errors=\"ignore\")\n",
    "            text = normalize_ws(text)\n",
    "\n",
    "            if len(text) < min_chars:\n",
    "                dropped_short += 1\n",
    "                continue\n",
    "\n",
    "            obj = {\"source\": \"wet\", \"url\": url, \"host\": host_from_url(url), \"content\": text}\n",
    "            out.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "            kept += 1\n",
    "\n",
    "    print(\"\\n✅ Extraction complete\")\n",
    "    print(\"Total records      :\", total)\n",
    "    print(\"Conversion records :\", conv)\n",
    "    print(\"Kept docs          :\", kept)\n",
    "    print(\"Dropped too short  :\", dropped_short)\n",
    "    print(\"Saved to           :\", out_jsonl)\n",
    "\n",
    "extract_wet_to_jsonl(WET_PATH, RAW_EXTRACT_JSONL, min_chars=MIN_CHARS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7eba39a",
   "metadata": {},
   "source": [
    "## JSONL iterator + basic reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4720cb7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'docs': 32879,\n",
       " 'length_chars': {'n': 32879,\n",
       "  'mean': 7480.538246297028,\n",
       "  'p50': 3807.0,\n",
       "  'p90': 13459.600000000002,\n",
       "  'max': 877216.0},\n",
       " 'top_hosts': [('cdha.cuny.edu', 14),\n",
       "  ('courseware.zcu.cz', 13),\n",
       "  ('turbotax.intuit.com', 10),\n",
       "  ('diecezja.pl', 9),\n",
       "  ('alcoholpolicy.niaaa.nih.gov', 8),\n",
       "  ('businessfig.com', 8),\n",
       "  ('www.besport.com', 8),\n",
       "  ('www.library.univ.kiev.ua', 7),\n",
       "  ('yscholarhub.yonsei.ac.kr', 6),\n",
       "  ('headquarters.s4.xrea.com', 5),\n",
       "  ('viavca.in2p3.fr', 5),\n",
       "  ('5ka-sale.ru', 5),\n",
       "  ('b-port.com', 5),\n",
       "  ('bryansk.news', 5),\n",
       "  ('ca.news.yahoo.com', 5),\n",
       "  ('andpremium.jp', 4),\n",
       "  ('arquivo.cienciaviva.pt', 4),\n",
       "  ('art.ceskatelevize.cz', 4),\n",
       "  ('burbujasweb.com', 4),\n",
       "  ('cleanindiajournal.com', 4)],\n",
       " 'host_concentration': {'top1_share': 0.0004258037044922291,\n",
       "  'top5_share': 0.0016423857173271693,\n",
       "  'top20_share': 0.004045135192676176}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def iter_jsonl(path: Path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                yield json.loads(line)\n",
    "\n",
    "def summarize_numeric(values):\n",
    "    arr = np.array(values, dtype=float) if values else np.array([], dtype=float)\n",
    "    if arr.size == 0:\n",
    "        return {\"n\": 0}\n",
    "    return {\n",
    "        \"n\": int(arr.size),\n",
    "        \"mean\": float(arr.mean()),\n",
    "        \"p50\": float(np.median(arr)),\n",
    "        \"p90\": float(np.quantile(arr, 0.9)),\n",
    "        \"max\": float(arr.max())\n",
    "    }\n",
    "\n",
    "def analyze_raw(jsonl_path: Path, top_k_hosts=20):\n",
    "    lengths = []\n",
    "    hosts = Counter()\n",
    "    for obj in iter_jsonl(jsonl_path):\n",
    "        t = obj.get(\"content\", \"\")\n",
    "        lengths.append(len(t))\n",
    "        hosts[obj.get(\"host\", \"\")] += 1\n",
    "\n",
    "    total = sum(hosts.values()) if hosts else 0\n",
    "    return {\n",
    "        \"docs\": len(lengths),\n",
    "        \"length_chars\": summarize_numeric(lengths),\n",
    "        \"top_hosts\": hosts.most_common(top_k_hosts),\n",
    "        \"host_concentration\": {\n",
    "            \"top1_share\": (hosts.most_common(1)[0][1] / total) if total else 0.0,\n",
    "            \"top5_share\": (sum(c for _, c in hosts.most_common(5)) / total) if total else 0.0,\n",
    "            \"top20_share\": (sum(c for _, c in hosts.most_common(20)) / total) if total else 0.0,\n",
    "        }\n",
    "    }\n",
    "\n",
    "raw_report = analyze_raw(RAW_EXTRACT_JSONL)\n",
    "raw_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009ed7ca",
   "metadata": {},
   "source": [
    "## Cleaning (line-based boilerplate removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "707329b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAV_LINE_RE = re.compile(r\"^(menu|home|about|contact|search|skip to|privacy|terms|login|sign in)$\", re.I)\n",
    "\n",
    "def clean_doc_text(text: str):\n",
    "    raw_lines = [ln.strip() for ln in text.splitlines()]\n",
    "    lines = [ln for ln in raw_lines if ln]\n",
    "\n",
    "    if not lines:\n",
    "        return \"\", 1.0\n",
    "\n",
    "    filtered = []\n",
    "    for ln in lines:\n",
    "        low = ln.lower().strip()\n",
    "        if len(low) <= 2:\n",
    "            continue\n",
    "        if len(low) <= 25 and NAV_LINE_RE.match(low):\n",
    "            continue\n",
    "        filtered.append(ln)\n",
    "\n",
    "    if not filtered:\n",
    "        return \"\", 1.0\n",
    "\n",
    "    cnt = Counter([ln.lower() for ln in filtered])\n",
    "    kept = [ln for ln in filtered if cnt[ln.lower()] < 3]\n",
    "\n",
    "    repeat_ratio = 1.0 - (len(set([ln.lower() for ln in kept])) / max(1, len(kept)))\n",
    "    cleaned = normalize_ws(\"\\n\".join(kept))\n",
    "    return cleaned, float(repeat_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df34875f",
   "metadata": {},
   "source": [
    "## NLTK-based tokenization + stopword ratio + quality metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98c41f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "DIGIT_RE = re.compile(r\"\\d\")\n",
    "PUNCT_RE = re.compile(r\"[^\\w\\s]\")\n",
    "\n",
    "def nltk_tokens(text: str):\n",
    "    # NLTK word tokenization\n",
    "    # Note: word_tokenize needs punkt.\n",
    "    return [t.lower() for t in word_tokenize(text)]\n",
    "\n",
    "def stopword_ratio_nltk(text: str):\n",
    "    toks = nltk_tokens(text)\n",
    "    # Keep alphabetic tokens only for stopword ratio stability\n",
    "    words = [t for t in toks if t.isalpha()]\n",
    "    if not words:\n",
    "        return 0.0\n",
    "    sw = sum(1 for w in words if w in EN_STOPWORDS)\n",
    "    return sw / len(words)\n",
    "\n",
    "def basic_ratios(text: str):\n",
    "    if not text:\n",
    "        return {\"nonascii\": 1.0, \"digit\": 1.0, \"punct\": 1.0}\n",
    "    n = len(text)\n",
    "    nonascii = sum(1 for c in text if ord(c) > 127) / n\n",
    "    digit = len(DIGIT_RE.findall(text)) / n\n",
    "    punct = len(PUNCT_RE.findall(text)) / n\n",
    "    return {\"nonascii\": nonascii, \"digit\": digit, \"punct\": punct}\n",
    "\n",
    "def is_english_langid(text: str):\n",
    "    lang, score = langid.classify(text[:5000])\n",
    "    return (lang == \"en\"), {\"lang\": lang, \"lang_score\": float(score)}\n",
    "\n",
    "def quality_pass(text: str, repeat_line_ratio: float):\n",
    "    if len(text) < MIN_CHARS:\n",
    "        return False, {\"fail\": \"too_short\"}\n",
    "\n",
    "    r = basic_ratios(text)\n",
    "    swr = stopword_ratio_nltk(text)\n",
    "\n",
    "    if r[\"nonascii\"] > MAX_NONASCII_RATIO:\n",
    "        return False, {\"fail\":\"nonascii\", **r, \"stopword_ratio\": swr, \"repeat_line_ratio\": repeat_line_ratio}\n",
    "    if r[\"digit\"] > MAX_DIGIT_RATIO:\n",
    "        return False, {\"fail\":\"digit\", **r, \"stopword_ratio\": swr, \"repeat_line_ratio\": repeat_line_ratio}\n",
    "    if r[\"punct\"] > MAX_PUNCT_RATIO:\n",
    "        return False, {\"fail\":\"punct\", **r, \"stopword_ratio\": swr, \"repeat_line_ratio\": repeat_line_ratio}\n",
    "    if swr < MIN_STOPWORD_RATIO:\n",
    "        return False, {\"fail\":\"low_stopwords\", **r, \"stopword_ratio\": swr, \"repeat_line_ratio\": repeat_line_ratio}\n",
    "    if repeat_line_ratio > MAX_REPEAT_LINE_RATIO:\n",
    "        return False, {\"fail\":\"too_repetitive\", **r, \"stopword_ratio\": swr, \"repeat_line_ratio\": repeat_line_ratio}\n",
    "\n",
    "    return True, {**r, \"stopword_ratio\": swr, \"repeat_line_ratio\": repeat_line_ratio}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d00e29",
   "metadata": {},
   "source": [
    "## Clean + English + quality filter → wet_cleaned_filtered.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d43b7563",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning+Filtering: 32879it [03:08, 173.99it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'drop_non_en': 19166,\n",
       "         'kept': 13436,\n",
       "         'drop_quality_low_stopwords': 223,\n",
       "         'drop_quality_digit': 51,\n",
       "         'drop_quality_nonascii': 3})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def filter_and_clean(in_jsonl: Path, out_jsonl: Path):\n",
    "    stats = Counter()\n",
    "\n",
    "    with open(out_jsonl, \"w\", encoding=\"utf-8\") as out:\n",
    "        for obj in tqdm(iter_jsonl(in_jsonl), desc=\"Cleaning+Filtering\"):\n",
    "            content = obj.get(\"content\", \"\")\n",
    "            if not content:\n",
    "                stats[\"drop_empty\"] += 1\n",
    "                continue\n",
    "\n",
    "            cleaned, rep = clean_doc_text(content)\n",
    "            if not cleaned:\n",
    "                stats[\"drop_clean_empty\"] += 1\n",
    "                continue\n",
    "\n",
    "            ok_lang, lang_meta = is_english_langid(cleaned)\n",
    "            if not ok_lang:\n",
    "                stats[\"drop_non_en\"] += 1\n",
    "                continue\n",
    "\n",
    "            ok_q, q_meta = quality_pass(cleaned, rep)\n",
    "            if not ok_q:\n",
    "                stats[f\"drop_quality_{q_meta.get('fail','unknown')}\"] += 1\n",
    "                continue\n",
    "\n",
    "            out_obj = {\n",
    "                \"url\": obj.get(\"url\", \"\"),\n",
    "                \"host\": obj.get(\"host\", \"\"),\n",
    "                \"source\": obj.get(\"source\", \"wet\"),\n",
    "                \"content\": cleaned,\n",
    "                \"meta\": {\"lang\": lang_meta, \"quality\": q_meta}\n",
    "            }\n",
    "            out.write(json.dumps(out_obj, ensure_ascii=False) + \"\\n\")\n",
    "            stats[\"kept\"] += 1\n",
    "\n",
    "    return stats\n",
    "\n",
    "filter_stats = filter_and_clean(RAW_EXTRACT_JSONL, CLEANED_JSONL)\n",
    "filter_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d79e5c",
   "metadata": {},
   "source": [
    "## Exact dedup → wet_deduped.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dd5eaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exact dedup: 13436it [00:03, 3910.88it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'kept': 13406, 'unique_hashes': 13406, 'drop_exact_dup': 30})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def content_hash(text: str) -> str:\n",
    "    t = normalize_ws(text)\n",
    "    return hashlib.md5(t.encode(\"utf-8\", errors=\"ignore\")).hexdigest()\n",
    "\n",
    "def exact_dedup(in_jsonl: Path, out_jsonl: Path):\n",
    "    seen = set()\n",
    "    stats = Counter()\n",
    "\n",
    "    with open(out_jsonl, \"w\", encoding=\"utf-8\") as out:\n",
    "        for obj in tqdm(iter_jsonl(in_jsonl), desc=\"Exact dedup\"):\n",
    "            t = obj.get(\"content\", \"\")\n",
    "            h = content_hash(t)\n",
    "            if h in seen:\n",
    "                stats[\"drop_exact_dup\"] += 1\n",
    "                continue\n",
    "            seen.add(h)\n",
    "            out.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "            stats[\"kept\"] += 1\n",
    "\n",
    "    stats[\"unique_hashes\"] = len(seen)\n",
    "    return stats\n",
    "\n",
    "dedup_stats = exact_dedup(CLEANED_JSONL, DEDUPED_JSONL)\n",
    "dedup_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e64310e",
   "metadata": {},
   "source": [
    "## Language distribution sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91135e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LangID dist: 100%|██████████| 20000/20000 [01:11<00:00, 279.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total classified: 20000\n",
      "Top 15 languages: [('en', 8266), ('ru', 1281), ('de', 1140), ('es', 1057), ('ja', 1037), ('fr', 992), ('zh', 902), ('it', 624), ('pt', 444), ('nl', 423), ('pl', 409), ('la', 292), ('cs', 253), ('id', 252), ('vi', 244)]\n"
     ]
    }
   ],
   "source": [
    "def lang_distribution(jsonl_path: Path, sample_n=20000, seed=42):\n",
    "    cnt = Counter()\n",
    "    rows = list(iter_jsonl(jsonl_path))\n",
    "    random.Random(seed).shuffle(rows)\n",
    "    rows = rows[:min(sample_n, len(rows))]\n",
    "\n",
    "    for obj in tqdm(rows, desc=\"LangID dist\"):\n",
    "        text = obj.get(\"content\", \"\")\n",
    "        if not text:\n",
    "            continue\n",
    "        lang, score = langid.classify(text[:5000])\n",
    "        cnt[lang] += 1\n",
    "\n",
    "    print(\"Total classified:\", sum(cnt.values()))\n",
    "    print(\"Top 15 languages:\", cnt.most_common(15))\n",
    "    return cnt\n",
    "\n",
    "lang_cnt = lang_distribution(RAW_EXTRACT_JSONL, sample_n=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f565508",
   "metadata": {},
   "source": [
    "## Save a report JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d2b2ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved report: data-v2/wet_report.json\n"
     ]
    }
   ],
   "source": [
    "post_report = analyze_raw(DEDUPED_JSONL)\n",
    "\n",
    "final_report = {\n",
    "    \"input_wet\": str(WET_PATH),\n",
    "    \"raw_extract_jsonl\": str(RAW_EXTRACT_JSONL),\n",
    "    \"cleaned_jsonl\": str(CLEANED_JSONL),\n",
    "    \"deduped_jsonl\": str(DEDUPED_JSONL),\n",
    "    \"raw_report\": raw_report,\n",
    "    \"filter_stats\": dict(filter_stats),\n",
    "    \"exact_dedup_stats\": dict(dedup_stats),\n",
    "    \"post_report\": post_report,\n",
    "    \"notes\": {\n",
    "        \"tokenization\": \"NLTK word_tokenize + NLTK stopwords used for stopword_ratio\",\n",
    "        \"language_id\": \"langid\",\n",
    "        \"dedup\": \"md5 of whitespace-normalized content\"\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(REPORT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(final_report, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✅ Saved report:\", REPORT_JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ec9939",
   "metadata": {},
   "source": [
    "## Near-duplicate detection (MinHash + LSH)\n",
    "\n",
    "This section identifies *near-duplicate* documents (e.g., mirrors, syndicated copies, template variants) that are not caught by exact MD5 deduplication.\n",
    "\n",
    "**Method:**  \n",
    "- Convert each document into word-shingles (default 5-grams)  \n",
    "- Build a MinHash signature per document  \n",
    "- Use LSH to retrieve candidate near-duplicates efficiently  \n",
    "- Cluster using union-find (transitive closure)\n",
    "\n",
    "> For class/demo purposes, the code defaults to running on a **sample** of the deduped dataset to keep runtime reasonable.  \n",
    "> Set `NEAR_DUP_SAMPLE_N = None` to run on all documents (may take longer).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c65b3efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs for near-dup: 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MinHash build: 100%|██████████| 3000/3000 [00:17<00:00, 169.00it/s]\n",
      "LSH query: 100%|██████████| 3000/3000 [00:00<00:00, 180775.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Near-dup edges: 4\n",
      "Clusters (size>1): 2\n",
      "Top cluster sizes: [3, 2]\n",
      "\n",
      "==========================================================================================\n",
      "Cluster 1 | size=3\n",
      "------------------------------------------------------------------------------------------\n",
      "URL: https://turbotax.intuit.com/reviews/online/deluxe/?page=10982\n",
      "TurboTax® Deluxe 2023-2024 - Customer Reviews - Page 10982 Skip To Main Content Only from TurboTax - file 100% FREE with expert help File 100% FREE with expert help ~37% of filers qualify. Form 1040 + limited credits only. Must file by 3/31. Start for free expand navigation optio ...\n",
      "------------------------------------------------------------------------------------------\n",
      "URL: https://turbotax.intuit.com/reviews/online/?page=9711\n",
      "TurboTax® Online 2023-2024 - Customer Reviews - Page 9711 Skip To Main Content Only from TurboTax - file 100% FREE with expert help File 100% FREE with expert help ~37% of filers qualify. Form 1040 + limited credits only. Must file by 3/31. Start for free expand navigation option ...\n",
      "------------------------------------------------------------------------------------------\n",
      "URL: https://turbotax.intuit.com/reviews/online/deluxe/?page=11873\n",
      "TurboTax® Deluxe 2023-2024 - Customer Reviews - Page 11873 Skip To Main Content Only from TurboTax - file 100% FREE with expert help File 100% FREE with expert help ~37% of filers qualify. Form 1040 + limited credits only. Must file by 3/31. Start for free expand navigation optio ...\n",
      "\n",
      "==========================================================================================\n",
      "Cluster 2 | size=2\n",
      "------------------------------------------------------------------------------------------\n",
      "URL: https://press.umich.edu/Contributors/J/Jin-Dal-Yong\n",
      "Dal Yong Jin | University of Michigan Press Skip to content Skip to menu Skip to menu Skip to search Skip to footer Michigan Publishing Brands Explore our publications and services. University of Michigan Press Publishes award-winning books that advance humanities and social scie ...\n",
      "------------------------------------------------------------------------------------------\n",
      "URL: https://press.umich.edu/Contributors/B/Boal-Dean\n",
      "Dean Boal | University of Michigan Press Skip to content Skip to menu Skip to menu Skip to search Skip to footer Michigan Publishing Brands Explore our publications and services. University of Michigan Press Publishes award-winning books that advance humanities and social science ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# If datasketch isn't installed, install it:\n",
    "# !pip -q install datasketch\n",
    "\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "\n",
    "# Controls\n",
    "NEAR_DUP_SAMPLE_N = 3000     # None = use all docs\n",
    "MINHASH_NUM_PERM  = 128\n",
    "LSH_THRESHOLD     = 0.90\n",
    "SHINGLE_SIZE      = 5\n",
    "\n",
    "WORD_RE = re.compile(r\"[A-Za-z]+\")\n",
    "\n",
    "def load_docs(jsonl_path: Path, limit=None, seed=42):\n",
    "    docs = list(iter_jsonl(jsonl_path))\n",
    "    if limit is not None and len(docs) > limit:\n",
    "        rng = random.Random(seed)\n",
    "        rng.shuffle(docs)\n",
    "        docs = docs[:limit]\n",
    "    return docs\n",
    "\n",
    "def word_shingles(text: str, k=5):\n",
    "    words = [w.lower() for w in WORD_RE.findall(text)]\n",
    "    if len(words) < k:\n",
    "        return []\n",
    "    return [\" \".join(words[i:i+k]) for i in range(len(words)-k+1)]\n",
    "\n",
    "def build_minhash(shingles, num_perm=128):\n",
    "    m = MinHash(num_perm=num_perm)\n",
    "    for s in shingles:\n",
    "        m.update(s.encode(\"utf-8\", errors=\"ignore\"))\n",
    "    return m\n",
    "\n",
    "class UnionFind:\n",
    "    def __init__(self, n):\n",
    "        self.p = list(range(n))\n",
    "        self.r = [0]*n\n",
    "    def find(self, x):\n",
    "        while self.p[x] != x:\n",
    "            self.p[x] = self.p[self.p[x]]\n",
    "            x = self.p[x]\n",
    "        return x\n",
    "    def union(self, a, b):\n",
    "        ra, rb = self.find(a), self.find(b)\n",
    "        if ra == rb: \n",
    "            return\n",
    "        if self.r[ra] < self.r[rb]:\n",
    "            self.p[ra] = rb\n",
    "        elif self.r[ra] > self.r[rb]:\n",
    "            self.p[rb] = ra\n",
    "        else:\n",
    "            self.p[rb] = ra\n",
    "            self.r[ra] += 1\n",
    "\n",
    "docs_nd = load_docs(DEDUPED_JSONL, limit=NEAR_DUP_SAMPLE_N, seed=RANDOM_SEED)\n",
    "print(\"Docs for near-dup:\", len(docs_nd))\n",
    "\n",
    "lsh = MinHashLSH(threshold=LSH_THRESHOLD, num_perm=MINHASH_NUM_PERM)\n",
    "minhashes = []\n",
    "\n",
    "for i, d in enumerate(tqdm(docs_nd, desc=\"MinHash build\")):\n",
    "    sh = word_shingles(d[\"content\"], SHINGLE_SIZE)\n",
    "    m  = build_minhash(sh, num_perm=MINHASH_NUM_PERM)\n",
    "    lsh.insert(str(i), m)\n",
    "    minhashes.append(m)\n",
    "\n",
    "uf = UnionFind(len(docs_nd))\n",
    "edges = 0\n",
    "\n",
    "for i, m in enumerate(tqdm(minhashes, desc=\"LSH query\")):\n",
    "    hits = lsh.query(m)\n",
    "    for h in hits:\n",
    "        j = int(h)\n",
    "        if j <= i:\n",
    "            continue\n",
    "        uf.union(i, j)\n",
    "        edges += 1\n",
    "\n",
    "comp = defaultdict(list)\n",
    "for i in range(len(docs_nd)):\n",
    "    comp[uf.find(i)].append(i)\n",
    "\n",
    "clusters = [v for v in comp.values() if len(v) > 1]\n",
    "clusters.sort(key=len, reverse=True)\n",
    "\n",
    "print(\"Near-dup edges:\", edges)\n",
    "print(\"Clusters (size>1):\", len(clusters))\n",
    "print(\"Top cluster sizes:\", [len(c) for c in clusters[:10]])\n",
    "\n",
    "# Show a few example clusters\n",
    "for ci, c in enumerate(clusters[:3], 1):\n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(f\"Cluster {ci} | size={len(c)}\")\n",
    "    for idx in c[:3]:\n",
    "        d = docs_nd[idx]\n",
    "        print(\"-\"*90)\n",
    "        print(\"URL:\", d.get(\"url\",\"\"))\n",
    "        print(d.get(\"content\",\"\")[:280].replace(\"\\n\",\" \"), \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9156469c",
   "metadata": {},
   "source": [
    "## Topic modeling (TF-IDF + NMF)\n",
    "\n",
    "This section provides an interpretable view of corpus themes using:\n",
    "- TF-IDF features (unigrams + bigrams)\n",
    "- NMF topic model (interpretable topic-word lists)\n",
    "\n",
    "> For class/demo purposes, it defaults to a **sample** of documents.  \n",
    "> Set `TOPIC_SAMPLE_N = None` to use all documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "faf22d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs for topic modeling: 5000\n",
      "\n",
      "Topic terms:\n",
      "topic_0 : ['news', 'services', 'business', 'contact', 'home', 'new', '2024', 'health', 'events', 'information', 'data', 'people']\n",
      "topic_1 : ['october', 'september', 'november', 'december', 'january', 'april', 'february', 'july', 'march', 'june', 'august', '2021']\n",
      "topic_2 : ['cookies', 'cookie', 'consent', 'website', 'necessary', 'cookie set', 'gdpr cookie', 'set gdpr', 'months cookie', 'user', '11 months', 'gdpr']\n",
      "topic_3 : ['redirect notice', 'redirect', 'previous page', '3d', 'page', 'notice', 'sending http', 'page sending', 'notice redirect', 'notice previous', 'visit page', 'page return']\n",
      "topic_4 : ['00', 'cart', 'price', 'shop', 'sale', 'shipping', 'accessories', 'add', 'product', 'products', '99', 'add cart']\n",
      "topic_5 : ['function', 'var', 'return', 'function var', 'function return', 'null', 'length', 'data', 'css', 'document', 'left', 'window']\n",
      "topic_6 : ['phpbb', 'forum', 'board', 'search', 'login', 'forums', 'register', 'password', 'topics', 'board index', 'posts', 'index']\n",
      "topic_7 : ['usd', 'islands', 'eur', 'republic', 'united', 'guinea', 'saint', 'south', 'fr', 'island', 'states', 'french']\n",
      "\n",
      "Example documents per topic (first 2):\n",
      "\n",
      "==========================================================================================\n",
      "Topic 0: ['news', 'services', 'business', 'contact', 'home', 'new', '2024', 'health', 'events', 'information', 'data', 'people']\n",
      "------------------------------------------------------------------------------------------\n",
      "URL: https://plantsociology.arphahub.com/browse_journal_articles_by_author?user_id=67539\n",
      "Articles by Author About PensoftBooksJournalsNews & BlogContact Register | Login Full Text Author Title Submit manuscript About Articles Issues Topical collections Author Guidelines Editorial Team Contacts Author Valentina L.A. Laface 1 article by this author  ...\n",
      "------------------------------------------------------------------------------------------\n",
      "URL: https://assets1.punchbowl.com/vendors/nj-new-jersey/avenel/c-limo?page=4\n",
      "Page 4 of 360 Avenel Limo - limousine, travel, chauffeur services in Avenel, New Jersey Punchbowl Online Invitations Digital Cards More Plans & Pricing Reminders Vendors Party Ideas Gift Cards Reasons to Celebrate Get Help You Account My Invitations My Digital ...\n",
      "\n",
      "==========================================================================================\n",
      "Topic 1: ['october', 'september', 'november', 'december', 'january', 'april', 'february', 'july', 'march', 'june', 'august', '2021']\n",
      "------------------------------------------------------------------------------------------\n",
      "URL: https://blog.ebrpl.com/01/2018/the-artists-way-13-week-program/\n",
      "The Artist’s Way – 13-week Program – East Baton Rouge Parish Library InfoBlog Skip to content East Baton Rouge Parish Library InfoBlog Interesting and useful websites, online resources, and search hints from the Library Information Service of the East Baton Ro ...\n",
      "------------------------------------------------------------------------------------------\n",
      "URL: https://diehardgamefan.com/2013/06/20/e3-2013-impressions-bayonetta-2-nintendo-wii-u/\n",
      "E3 2013 Impressions: Bayonetta 2 (Nintendo Wii U) – Diehard GameFAN Diehard GameFAN 30 Days of Dreamcast About Us admin Contact Us E3 2010 E3 2011 Live Coverage home Home 2015 images Latest Updates loader More Comments Pokemon Week 2010 Poll Archive Review Arc ...\n",
      "\n",
      "==========================================================================================\n",
      "Topic 2: ['cookies', 'cookie', 'consent', 'website', 'necessary', 'cookie set', 'gdpr cookie', 'set gdpr', 'months cookie', 'user', '11 months', 'gdpr']\n",
      "------------------------------------------------------------------------------------------\n",
      "URL: https://pixelverse.fun/the-last-of-us-part-ii-review-emotional-journey-in-a-post-apocalyptic-world/\n",
      "The Last Of Us Part II Review - Emotional Journey In A Post-Apocalyptic World - Pixelverse Skip to content No results Home FAQ’s Game Reviews About Us Disclosure Privacy Policy Terms Of Use Contact Us Home FAQ’s Game Reviews About Us Disclosure Privacy Policy  ...\n",
      "------------------------------------------------------------------------------------------\n",
      "URL: https://www.lifecoachmagazine.com/willpower-does-not-last/?fbclid=IwAR1Y5aNGk4J9Fn-_7rjznJHu1apQvkbIfGhVR4c6JZRdiWKeVY65J5Pdzm4\n",
      "Willpower Does Not Last—Imagination is the Solution | Life Coach Magazine Press \"Enter\" to skip to content Search Search Life Coach Magazine Subscribe Write for us Log in Create Profile open menu Back Home Coach Training & Certification Coaching Software Perso ...\n",
      "\n",
      "==========================================================================================\n",
      "Topic 3: ['redirect notice', 'redirect', 'previous page', '3d', 'page', 'notice', 'sending http', 'page sending', 'notice redirect', 'notice previous', 'visit page', 'page return']\n",
      "------------------------------------------------------------------------------------------\n",
      "URL: https://maps.google.hu/url?q=http%3A%2F%2Fcserepeslemez-tetofedes.komplexwebrent.eu%2Fmicroblog-bejegyzes%2Fmenyhert-tetofedo%2Fdunakeszi%2FJThDJTA5JUQzJTVDJUE5JUZEWiUxRTFIJUI3JUYwJTg3JTkxJUMwXw%253D%253D%2FJUM4JTgxJURBJTExJTBEJUNDJUUyJUQ3JTFGJTk3YyU4NiVBNG42JUIy%2F\n",
      "Redirect Notice Redirect Notice The previous page is sending you to http://cserepeslemez-tetofedes.komplexwebrent.eu/microblog-bejegyzes/menyhert-tetofedo/dunakeszi/JThDJTA5JUQzJTVDJUE5JUZEWiUxRTFIJUI3JUYwJTg3JTkxJUMwXw%3D%3D/JUM4JTgxJURBJTExJTBEJUNDJUUyJUQ3JT ...\n",
      "------------------------------------------------------------------------------------------\n",
      "URL: https://www.google.gr/url?q=http%3A%2F%2Fwebaruhaz-marketing.komplexweb-keresomarketing.hu%2Fszolgaltatas%2Fkreativ-ugynokseg-berelheto-weboldal-keszites%2FJTgzJUQ3JUJCLlklMDMlODF3JThCWiUwNSU5OSVEOCVDQ3glOTI%253D%2F\n",
      "Redirect Notice Redirect Notice The previous page is sending you to http://webaruhaz-marketing.komplexweb-keresomarketing.hu/szolgaltatas/kreativ-ugynokseg-berelheto-weboldal-keszites/JTgzJUQ3JUJCLlklMDMlODF3JThCWiUwNSU5OSVEOCVDQ3glOTI%3D/. If you do not want  ...\n",
      "\n",
      "==========================================================================================\n",
      "Topic 4: ['00', 'cart', 'price', 'shop', 'sale', 'shipping', 'accessories', 'add', 'product', 'products', '99', 'add cart']\n",
      "------------------------------------------------------------------------------------------\n",
      "URL: https://www.jayscarpetonederidder.com/flooring/p/branden-pier\n",
      "﻿Tigressa® Cherish Branden Pier | Jay's Carpet One Floor & Home Skip Navigation Wednesday Hours: | 337-202-5849 Products All Flooring Hardwood Luxury Vinyl Carpet Laminate Tile Floor Care Resources Flooring Guide Blog About Us About Us Room by Room The Beautif ...\n",
      "------------------------------------------------------------------------------------------\n",
      "URL: http://recallformoms.com/Sports-Outdoors-Recreation/Sports-Equipment/Stamina-Products-to-Pay-105,000-Civil-Penalty-for-Failure-to-Report-Defects-with-Mini-Trampolines.html\n",
      "Product Recalls for Moms: Stamina Products to Pay $105,000 Civil Penalty for Failure to Report Defects with Mini-Trampolines Recall for Moms Product Recall Information for Moms New | Advisory/Informational | Furniture Infant/Toddler Play Pens, High Chairs, Swi ...\n",
      "\n",
      "==========================================================================================\n",
      "Topic 5: ['function', 'var', 'return', 'function var', 'function return', 'null', 'length', 'data', 'css', 'document', 'left', 'window']\n",
      "------------------------------------------------------------------------------------------\n",
      "URL: https://adultnichezone.com/video/20302/ayumi-anime-humps-the-cooch-of-scarlett-sage-with-a-strap-on/\n",
      "Ayumi Anime humps the cooch of Scarlett Sage with a strap-on | Exclusive online sex productions for free on the finest tube in town. Fuck milf Home Models Categories Ayumi Anime humps the cooch of Scarlett Sage with a strap-on Your browser does not support the ...\n",
      "------------------------------------------------------------------------------------------\n",
      "URL: https://www.roseindia.net/answers/viewqa/Java-Beginners/22076-Hi-.html\n",
      "Hi Java Java - Index Core Java Java SE 6 Advanced Java JSP Servlets JDBC EJB Web Services J2ME JSTL Frameworks Frameworks - Index Hibernate Struts 1.x Struts 2 JSF JavaFX Ajax Spring 2.5 Spring 3 DOJO iBatis Flex 3 Flex 4 Database Database - Index SQL MySQL Te ...\n",
      "\n",
      "==========================================================================================\n",
      "Topic 6: ['phpbb', 'forum', 'board', 'search', 'login', 'forums', 'register', 'password', 'topics', 'board index', 'posts', 'index']\n",
      "------------------------------------------------------------------------------------------\n",
      "URL: https://mavicpilots.com/tags/new-to-drone-flying/\n",
      "new to drone flying | DJI Mavic, Air & Mini Drone Community Menu Forums New posts Search forums New New posts New media New media comments New profile posts Latest activity Popular Most Replied Threads Most Liked Threads Most Viewed Threads Most Liked Posts Wi ...\n",
      "------------------------------------------------------------------------------------------\n",
      "URL: https://www.happycow.net/reviews/wellness-warehouse-cape-town-105734\n",
      "Wellness Warehouse - Helderberg - Cape Town Health Store - HappyCow Looks like your browser doesn't support JavaScript. HappyCow may not work without JavaScript enabled. Explore Nearby Top Rated B&B Retreats Add a Listing Write Review Forum Blog Community The  ...\n",
      "\n",
      "==========================================================================================\n",
      "Topic 7: ['usd', 'islands', 'eur', 'republic', 'united', 'guinea', 'saint', 'south', 'fr', 'island', 'states', 'french']\n",
      "------------------------------------------------------------------------------------------\n",
      "URL: https://wimbiclothingco.com/collections/frontpage/products/wimbi-hoodie\n",
      "Wimbi Hoodie – Wimbi Clothing Co Home What's New? T-Shirts Hoodies and Sweatshirts Headwear Jackets Bottoms Search Instagram Facebook Your cart Close Cart Site navigation Home What's New? T-Shirts Hoodies and Sweatshirts Headwear Jackets Bottoms Search Cart Ca ...\n",
      "------------------------------------------------------------------------------------------\n",
      "URL: https://ror.shoes/products/nike-air-max-97-overbranding-black\n",
      "Nike Air Max 97 \"Overbranding Black\" | Retail Or Resell Skip to content Search × × Search Search loading Menu × × Register Log in Upcoming Releases New Arrivals Footwear View All Footwear Men's Footwear Women's Footwear Nike Nike Dunk Low Nike Dunk High Adidas ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Controls\n",
    "TOPIC_SAMPLE_N = 5000    # None = use all docs\n",
    "N_TOPICS       = 8\n",
    "TOP_TERMS      = 12\n",
    "\n",
    "docs_tm = load_docs(DEDUPED_JSONL, limit=TOPIC_SAMPLE_N, seed=RANDOM_SEED)\n",
    "texts = [d[\"content\"] for d in docs_tm]\n",
    "print(\"Docs for topic modeling:\", len(texts))\n",
    "\n",
    "if len(texts) < 20:\n",
    "    print(\"Not enough documents to run topic modeling reliably.\")\n",
    "else:\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=30000,\n",
    "        min_df=2,\n",
    "        max_df=0.9,\n",
    "        ngram_range=(1,2),\n",
    "        stop_words=\"english\"\n",
    "    )\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "\n",
    "    nmf = NMF(n_components=N_TOPICS, random_state=RANDOM_SEED)\n",
    "    W = nmf.fit_transform(X)\n",
    "    H = nmf.components_\n",
    "    terms = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "    def top_terms(topic_idx, topn=12):\n",
    "        idx = np.argsort(H[topic_idx])[::-1][:topn]\n",
    "        return terms[idx].tolist()\n",
    "\n",
    "    topic_terms = {f\"topic_{i}\": top_terms(i, TOP_TERMS) for i in range(N_TOPICS)}\n",
    "    dominant = W.argmax(axis=1)\n",
    "\n",
    "    print(\"\\nTopic terms:\")\n",
    "    for k, v in topic_terms.items():\n",
    "        print(k, \":\", v)\n",
    "\n",
    "    print(\"\\nExample documents per topic (first 2):\")\n",
    "    for t in range(N_TOPICS):\n",
    "        idxs = np.where(dominant == t)[0][:2]\n",
    "        if len(idxs) == 0:\n",
    "            continue\n",
    "        print(\"\\n\" + \"=\"*90)\n",
    "        print(f\"Topic {t}: {topic_terms[f'topic_{t}']}\")\n",
    "        for i in idxs:\n",
    "            print(\"-\"*90)\n",
    "            print(\"URL:\", docs_tm[i].get(\"url\",\"\"))\n",
    "            print(docs_tm[i][\"content\"][:260].replace(\"\\n\",\" \"), \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4db902b",
   "metadata": {},
   "source": [
    "## NLTK vs Regex demo (for class discussion)\n",
    "\n",
    "This cell demonstrates how **NLTK tokenization** differs from a simple **regex tokenization** approach on real web text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30ff6f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGEX TOKENS (first 60):\n",
      "['articles', 'by', 'author', 'about', 'pensoftbooksjournalsnews', 'blogcontact', 'register', 'login', 'full', 'text', 'author', 'title', 'submit', 'manuscript', 'about', 'articles', 'issues', 'topical', 'collections', 'author', 'guidelines', 'editorial', 'team', 'contacts', 'author', 'valentina', 'l', 'a', 'laface', 'article', 'by', 'this', 'author', 'sort', 'by', 'publication', 'date', 'newest', 'publication', 'date', 'oldest', 'total', 'views', 'unique', 'views', 'best', 'match', 'citations', 'counthighly', 'accessed', 'last', 'month', 'highly', 'accessed', 'last', 'months', 'highly', 'accessed', 'last', 'months']\n",
      "\n",
      "NLTK TOKENS (first 60):\n",
      "['articles', 'by', 'author', 'about', 'pensoftbooksjournalsnews', '&', 'blogcontact', 'register', '|', 'login', 'full', 'text', 'author', 'title', 'submit', 'manuscript', 'about', 'articles', 'issues', 'topical', 'collections', 'author', 'guidelines', 'editorial', 'team', 'contacts', 'author', 'valentina', 'l.a.', 'laface', '1', 'article', 'by', 'this', 'author', 'sort', 'by', ':', 'publication', 'date', 'newest', 'publication', 'date', 'oldest', 'total', 'views', 'unique', 'views', 'best', 'match', 'citations', 'counthighly', 'accessed', '(', 'last', 'month', ')', 'highly', 'accessed', '(']\n",
      "\n",
      "Notes:\n",
      "- NLTK keeps punctuation as separate tokens; regex strips it.\n",
      "- NLTK handles contractions and punctuation boundaries more explicitly.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Pick one random English document from the deduped set\n",
    "docs_demo = load_docs(DEDUPED_JSONL, limit=200, seed=RANDOM_SEED)\n",
    "sample_doc = docs_demo[0][\"content\"]\n",
    "\n",
    "# Regex tokenization (simple)\n",
    "regex_tokens = re.findall(r\"[A-Za-z]+\", sample_doc.lower())[:60]\n",
    "\n",
    "# NLTK tokenization (word_tokenize)\n",
    "nltk_tok = [t.lower() for t in word_tokenize(sample_doc)][:60]\n",
    "\n",
    "print(\"REGEX TOKENS (first 60):\")\n",
    "print(regex_tokens)\n",
    "\n",
    "print(\"\\nNLTK TOKENS (first 60):\")\n",
    "print(nltk_tok)\n",
    "\n",
    "print(\"\\nNotes:\")\n",
    "print(\"- NLTK keeps punctuation as separate tokens; regex strips it.\")\n",
    "print(\"- NLTK handles contractions and punctuation boundaries more explicitly.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
